{
    "docs": [
        {
            "location": "/index.html", 
            "text": "taco\n is a library for compiling dense and sparse linear and tensor algebra expressions. The expressions can range from simple kernels like SpMV to more complex kernels like MTTKRP, where the operands can be dense, sparse, or a mix of dense and sparse. taco automatically generates efficient compute kernels (loops) to evaluate these expressions.\n\n\nThe sidebar to the left links to documentation for the taco C++ library as well as some examples demonstrating how taco can be used in real-world applications.\n\n\nSystem Requirements\n\n\n\n\nA C compiler that supports C99 and OpenMP (if parallelism is desired), such as GCC or Clang\n\n\n\n\nGetting Help\n\n\nQuestions and bug reports can be submitted \nhere\n.", 
            "title": "Home"
        }, 
        {
            "location": "/index.html#system-requirements", 
            "text": "A C compiler that supports C99 and OpenMP (if parallelism is desired), such as GCC or Clang", 
            "title": "System Requirements"
        }, 
        {
            "location": "/index.html#getting-help", 
            "text": "Questions and bug reports can be submitted  here .", 
            "title": "Getting Help"
        }, 
        {
            "location": "/pytensors/index.html", 
            "text": "Declaring Tensors\n\n\npytaco.Tensor\n objects correspond to mathematical tensors. You can can declare a new tensor by specifying its name, a vector with the size of each dimension and the \nstorage format\n that will be used to store the tensor and a \ndatatype\n:\n\n\n# Import the pytaco library\nimport pytaco as pt\n# Import the storage formats to save some typing\nfrom pytaco import dense, compressed\n\n# Declare a new tensor \nA\n of double-precision floats with dimensions \n# 512 x 64 x 2048, stored as a dense-sparse-sparse tensor\nA = pt.tensor(\nA\n, [512, 64, 2048], pt.format([dense, compressed, compressed]), pt.float64)\n\n\n\n\nThe name of the tensor can be omitted, in which case taco will assign an arbitrary name to the tensor:\n\n\nimport pytaco as pt\nfrom pytaco import dense, compressed\n\n# Declare a tensor with the same dimensions, storage format and type as before\nA = pt.tensor([512, 64, 2048], pt.format([dense, compressed, compressed]), pt.float64)\n\n\n\n\nThe \ndatatype\n can also be omitted in which case taco will default to using \npt.float32\n:\n\n\nimport pytaco as pt\nfrom pytaco import dense, compressed\n\n# Declare a tensor with the same dimensions and storage format as before\nA = pt.tensor([512, 64, 2048], pt.format([dense, compressed, compressed]))\n\n\n\n\nA single format can be given to create a tensor where all dimensions have that format:\n\n\nimport pytaco as pt\nfrom pytaco import dense, compressed\n\n# Declare a dense tensor\nA = pt.tensor([512, 64, 2048], dense)\n\n# Declare a compressed tensor\nB = pt.tensor([512, 64, 2048], compressed)\n\n\n\n\nScalars, which are treated as order-0 tensors, can be declared and initialized with some arbitrary value as demonstrated below:\n\n\nimport pytaco as pt\nfrom pytaco import dense, compressed\n\n# Declare a scalar\naplha = pt.tensor(42.0)\n\n\n\n\nDefining Tensor Formats\n\n\nConceptually, you can think of a tensor as a tree with each level (excluding the root) corresponding to a dimension of the tensor. Each path from the root to a leaf node represents a tensor coordinate and its corresponding value. Which dimension each level of the tree corresponds to is determined by the order in which dimensions of the tensor are stored.\n\n\ntaco uses a novel scheme that can describe different storage formats for any tensor by specifying the order in which tensor dimensions are stored and whether each dimension is sparse or dense. A sparse dimension stores only the subset of the dimension that contains non-zero values and is conceptually similar to the index arrays used in the compressed sparse row (CSR) matrix format, while a dense dimension stores both zeros and non-zeros. As demonstrated below, this scheme is flexibile enough to express many commonly-used matrix storage formats.\n\n\nYou can define a new tensor storage format by creating a \npytaco.format\n object. The constructor for \npytaco.format\n takes as arguments a list specifying the type of each dimension and (optionally) a list specifying the order in which dimensions are to be stored, as seen below:\n\n\nimport pytaco as pt\nfrom pytaco import dense, compressed, format\ndm   = format([dense, dense])                   # (Row-major) dense matrix\ncsr  = format([dense, compressed])              # Compressed sparse row matrix\ncsc  = format([dense, compressed], [1, 0])      # Compressed sparse column matrix\ndcsr = format([compressed, compressed], [1, 0]) # Doubly compressed sparse column matrix\n\n\n\n\npytaco\n provides common formats (csr, csc and csf) by default and can be used by simply typing \npt.csr\n, \npt.csc\n or \npt.csf\n.\n\n\nTensor Datatypes\n\n\nTensors can be of 10 different datatypes. The following are the possible tensor datatypes:\n\n\nSigned Integers:\n\n\npytaco.int8\n\n\npytaco.int16\n\n\npytaco.int32\n\n\npytaco.int64\n\n\nUnsigned Integers:\n\n\npytaco.uint8\n\n\npytaco.uint16\n\n\npytaco.uint32\n\n\npytaco.uint64\n\n\nFloating point precision: \n\n\npytaco.float32\n \n\n\npytaco.float\n\n\nDouble precision: \n\n\npytaco.float64\n \n\n\npytaco.double\n\n\nInitializing Tensors\n\n\nTensors can be made by using python indexing syntax. For example, one may write the following:\n\n\nimport pytaco as pt\nfrom pytaco import dense, compressed\n\n# Declare a dense tensor\nA = pt.tensor([512, 64, 2048], compressed)\n\n# Set location (0, 1, 0) in A to 42.0\nA[0, 1, 0] = 42.0\n\n\n\n\nThe insert operator adds the inserted non-zeros to a temporary buffer. Before a tensor can actually be used in a computation, it is automatcally packed. \n\n\nFor most cases, this is not necessary but you may also invoke the \npack\n method to compress the tensor into the storage format that was specified after all values have been inserted.\n\n\nNOTE: Multidimensional indexing (as used with lists) are NOT supported. For example, the following is invalid code:\n\n\nimport pytaco as pt\nfrom pytaco import dense, compressed\n\n# Declare a dense tensor\nA = pt.tensor([512, 64, 2048], compressed)\n\n# INVALID STATEMENT\nA[0][1][0] = 42.0\n\n\n\n\nLoading Tensors from File\n\n\nRather than manually invoking building a tensor, you can load tensors directly from file by calling \npytaco.read\n as demonstrated below:\n\n\nimport pytaco as pt\nfrom pytaco import dense, compressed, format\n\n# Load a dense-sparse-sparse tensor from file A.tns\nA = pt.read(\nA.tns\n, format([dense, compressed, compressed]))\n\n\n\n\nBy default, \npytaco.read\n returns a packed tensor. You can optionally pass a Boolean flag as an argument to indicate whether the returned tensor should be packed or not: \n\n\nimport pytaco as pt\nfrom pytaco import dense, compressed, format\n\n# Load an unpacked tensor from file A.tns\nA = pt.read(\nA.tns\n, format([dense, compressed, compressed]), false)\n\n\n\n\nNOTE: the tensor will be packed anyway before any computation is actually performed.\n\n\nCurrently, taco supports loading from the following matrix and tensor file formats:\n\n\n\n\nMatrix Market (Coordinate) Format (.mtx)\n\n\nRutherford-Boeing Format (.rb)\n\n\nFROSTT Format (.tns)\n\n\n\n\nWriting Tensors to Files\n\n\nYou can also write a (packed) tensor directly to file by calling \npytaco.write\n, as demonstrated below:\n\n\nimport pytaco as pt\n\nA = pt.tensor([512, 64, 2048], compressed)\nA[0, 1, 0] = 42.0\nA[1, 1, 1] = 77\npt.write(\nA.tns\n, A);  # Write tensor A to file A.tns\n\n\n\n\npytaco.write\n supports the same set of matrix and tensor file formats as \npytaco.read\n.\n\n\nI/O with Numpy or Scipy\n\n\nTensors can be initialized with either numpy arrays or scipy sparse CSC or CSR matrices. As such, we can use the I/O from numpy and scipy and feed the data into pytaco by initializing a tensor.\n\n\nimport pytaco as pt\nimport numpy as np\nimport scipy.sparse\n\n# Assuming matrix is CSR\nsparse_matrix = scipy.sparse.load_npz('sparse_matrix.npz')\n\n# Pass data into taco for use\ntaco_tensor = pt.from_scipy_csr(sparse_matrix)\n\n# We can also load a numpy array\nnp_array = np.load('arr.npy')\n\n# And initialize a tensor from this array\ndense_tensor = pt.from_numpy_array(np_array)", 
            "title": "Defining Tensors"
        }, 
        {
            "location": "/pytensors/index.html#declaring-tensors", 
            "text": "pytaco.Tensor  objects correspond to mathematical tensors. You can can declare a new tensor by specifying its name, a vector with the size of each dimension and the  storage format  that will be used to store the tensor and a  datatype :  # Import the pytaco library\nimport pytaco as pt\n# Import the storage formats to save some typing\nfrom pytaco import dense, compressed\n\n# Declare a new tensor  A  of double-precision floats with dimensions \n# 512 x 64 x 2048, stored as a dense-sparse-sparse tensor\nA = pt.tensor( A , [512, 64, 2048], pt.format([dense, compressed, compressed]), pt.float64)  The name of the tensor can be omitted, in which case taco will assign an arbitrary name to the tensor:  import pytaco as pt\nfrom pytaco import dense, compressed\n\n# Declare a tensor with the same dimensions, storage format and type as before\nA = pt.tensor([512, 64, 2048], pt.format([dense, compressed, compressed]), pt.float64)  The  datatype  can also be omitted in which case taco will default to using  pt.float32 :  import pytaco as pt\nfrom pytaco import dense, compressed\n\n# Declare a tensor with the same dimensions and storage format as before\nA = pt.tensor([512, 64, 2048], pt.format([dense, compressed, compressed]))  A single format can be given to create a tensor where all dimensions have that format:  import pytaco as pt\nfrom pytaco import dense, compressed\n\n# Declare a dense tensor\nA = pt.tensor([512, 64, 2048], dense)\n\n# Declare a compressed tensor\nB = pt.tensor([512, 64, 2048], compressed)  Scalars, which are treated as order-0 tensors, can be declared and initialized with some arbitrary value as demonstrated below:  import pytaco as pt\nfrom pytaco import dense, compressed\n\n# Declare a scalar\naplha = pt.tensor(42.0)", 
            "title": "Declaring Tensors"
        }, 
        {
            "location": "/pytensors/index.html#defining-tensor-formats", 
            "text": "Conceptually, you can think of a tensor as a tree with each level (excluding the root) corresponding to a dimension of the tensor. Each path from the root to a leaf node represents a tensor coordinate and its corresponding value. Which dimension each level of the tree corresponds to is determined by the order in which dimensions of the tensor are stored.  taco uses a novel scheme that can describe different storage formats for any tensor by specifying the order in which tensor dimensions are stored and whether each dimension is sparse or dense. A sparse dimension stores only the subset of the dimension that contains non-zero values and is conceptually similar to the index arrays used in the compressed sparse row (CSR) matrix format, while a dense dimension stores both zeros and non-zeros. As demonstrated below, this scheme is flexibile enough to express many commonly-used matrix storage formats.  You can define a new tensor storage format by creating a  pytaco.format  object. The constructor for  pytaco.format  takes as arguments a list specifying the type of each dimension and (optionally) a list specifying the order in which dimensions are to be stored, as seen below:  import pytaco as pt\nfrom pytaco import dense, compressed, format\ndm   = format([dense, dense])                   # (Row-major) dense matrix\ncsr  = format([dense, compressed])              # Compressed sparse row matrix\ncsc  = format([dense, compressed], [1, 0])      # Compressed sparse column matrix\ndcsr = format([compressed, compressed], [1, 0]) # Doubly compressed sparse column matrix  pytaco  provides common formats (csr, csc and csf) by default and can be used by simply typing  pt.csr ,  pt.csc  or  pt.csf .", 
            "title": "Defining Tensor Formats"
        }, 
        {
            "location": "/pytensors/index.html#tensor-datatypes", 
            "text": "Tensors can be of 10 different datatypes. The following are the possible tensor datatypes:  Signed Integers:  pytaco.int8  pytaco.int16  pytaco.int32  pytaco.int64  Unsigned Integers:  pytaco.uint8  pytaco.uint16  pytaco.uint32  pytaco.uint64  Floating point precision:   pytaco.float32    pytaco.float  Double precision:   pytaco.float64    pytaco.double", 
            "title": "Tensor Datatypes"
        }, 
        {
            "location": "/pytensors/index.html#initializing-tensors", 
            "text": "Tensors can be made by using python indexing syntax. For example, one may write the following:  import pytaco as pt\nfrom pytaco import dense, compressed\n\n# Declare a dense tensor\nA = pt.tensor([512, 64, 2048], compressed)\n\n# Set location (0, 1, 0) in A to 42.0\nA[0, 1, 0] = 42.0  The insert operator adds the inserted non-zeros to a temporary buffer. Before a tensor can actually be used in a computation, it is automatcally packed.   For most cases, this is not necessary but you may also invoke the  pack  method to compress the tensor into the storage format that was specified after all values have been inserted.  NOTE: Multidimensional indexing (as used with lists) are NOT supported. For example, the following is invalid code:  import pytaco as pt\nfrom pytaco import dense, compressed\n\n# Declare a dense tensor\nA = pt.tensor([512, 64, 2048], compressed)\n\n# INVALID STATEMENT\nA[0][1][0] = 42.0", 
            "title": "Initializing Tensors"
        }, 
        {
            "location": "/pytensors/index.html#loading-tensors-from-file", 
            "text": "Rather than manually invoking building a tensor, you can load tensors directly from file by calling  pytaco.read  as demonstrated below:  import pytaco as pt\nfrom pytaco import dense, compressed, format\n\n# Load a dense-sparse-sparse tensor from file A.tns\nA = pt.read( A.tns , format([dense, compressed, compressed]))  By default,  pytaco.read  returns a packed tensor. You can optionally pass a Boolean flag as an argument to indicate whether the returned tensor should be packed or not:   import pytaco as pt\nfrom pytaco import dense, compressed, format\n\n# Load an unpacked tensor from file A.tns\nA = pt.read( A.tns , format([dense, compressed, compressed]), false)  NOTE: the tensor will be packed anyway before any computation is actually performed.  Currently, taco supports loading from the following matrix and tensor file formats:   Matrix Market (Coordinate) Format (.mtx)  Rutherford-Boeing Format (.rb)  FROSTT Format (.tns)", 
            "title": "Loading Tensors from File"
        }, 
        {
            "location": "/pytensors/index.html#writing-tensors-to-files", 
            "text": "You can also write a (packed) tensor directly to file by calling  pytaco.write , as demonstrated below:  import pytaco as pt\n\nA = pt.tensor([512, 64, 2048], compressed)\nA[0, 1, 0] = 42.0\nA[1, 1, 1] = 77\npt.write( A.tns , A);  # Write tensor A to file A.tns  pytaco.write  supports the same set of matrix and tensor file formats as  pytaco.read .", 
            "title": "Writing Tensors to Files"
        }, 
        {
            "location": "/pytensors/index.html#io-with-numpy-or-scipy", 
            "text": "Tensors can be initialized with either numpy arrays or scipy sparse CSC or CSR matrices. As such, we can use the I/O from numpy and scipy and feed the data into pytaco by initializing a tensor.  import pytaco as pt\nimport numpy as np\nimport scipy.sparse\n\n# Assuming matrix is CSR\nsparse_matrix = scipy.sparse.load_npz('sparse_matrix.npz')\n\n# Pass data into taco for use\ntaco_tensor = pt.from_scipy_csr(sparse_matrix)\n\n# We can also load a numpy array\nnp_array = np.load('arr.npy')\n\n# And initialize a tensor from this array\ndense_tensor = pt.from_numpy_array(np_array)", 
            "title": "I/O with Numpy or Scipy"
        }, 
        {
            "location": "/pycomputations/index.html", 
            "text": "Specifying Tensor Algebra Computations\n\n\nTensor algebra computations can be expressed in taco with tensor index notation, which on a high level describes how each element in the output tensor can be computed from elements in the input tensors.  \nTensor index notation\n is described in detail in the description of the C++ library. Here, we focus on the syntax for python.\n\n\nThe syntax is very similar to C++ except that we use square brackets \n[]\n to access tensors instead of parenthesis. For instance, to express \nmatrix addition\n in Python, we simply need to write:\n\n\nA[i, j] = B[i, j] + C[i, j]\n\n\n\n\nwhere \nA\n, \nB\n, and \nC\n denote order-2 tensors (i.e. matrices) while \ni\n and \nj\n are index variables that represent abstract indices into the corresponding dimensions of the tensors as in the C++ API.\n\n\nIt is important to note that due to the complications that arise from assembling sparse structures, we cannot have a tensor appear both on the left hand side and the right hand side of an expression.\n\n\nIndexVars\n\n\nAs in the C++ API, index vars are needed to form index expressions. Tensors can be accessed with either a \npytaco.indexVar\n to express computations or integers to read or write to a location in a tensor. So for instance, before specifying the above we could write:\n\n\nimport pytaco as pt\n\n# First method to make index vars\ni, j = pt.indexVar(), pt.indexVar()\n\n\n\n\nWe can also give names to indexVars as shown below:\n\n\nimport pytaco as pt\n\n# Named indexVars\ni, j = pt.indexVar(\ni\n), pt.indexVar(\nj\n)\n\n\n\n\nTo make this less cumbersome, we also provide a function \npytaco.get_index_vars\n to generate a list of n different index vars that can be used in tensor expressions.\n\n\nimport pytaco as pt\ni, j = pt.get_index_vars(2)\n\n\n\n\nNOTE: When using scalars to express computations we must still use the square brackets to index the tensor. Since scalars are order-0 tensors, \nNone\n must be passed into the index to specify that no \nindexVar\ns are used:\n\n\nimport pytaco as pt\ni, j = pt.get_index_vars(2)\n\n# Make a scalar value\nA = pt.tensor(0)\n# Make a compressed tensor of size 3x3\nB = pt.tensor([3,3])\n\n# Make some assignments\nB[0, 0] = 1\nB[1, 0] = 10\n\n# We can sum the elements in B as follows. Notice we need to use None to tell \n# taco that A is a scalar.\nA[None] = B[i, j]\n\n\n\n\nBroadcasting\n\n\nWhen using \nindexVar\ns, we must ensure that dimensions with the same \nindexVar\n are of the same size. Operations can be broadcast along outer dimensions assuming the inner dimensions are of the same size. For example:\n\n\nimport pytaco as pt\ni, j, k = pt.get_index_vars(3)\n\n# Make a compressed tensor of size 3x3\nA = pt.tensor([3,3])\nB = pt.tensor([3,3])\n\n# Make a dense vector\nC = pt.tensor([3], pt.dense)\n\n# Make some assignments\nC[0] = 1\nB[0, 0] = 1\n\n# We can add C to each row of B as follows:\nA[i, j] =  B[i, j] + C[j]\n\n\n\n\nThe following, however, is not valid since the dimension of index j is of a different size for the different tensors:\n\n\nimport pytaco as pt\ni, j, k = pt.get_index_vars(3)\n\n# Make a compressed tensor of size 3x3\nA = pt.tensor([3,3])\nB = pt.tensor([3,3])\n\n# Make a dense vector\nC = pt.tensor([3,1], pt.dense)\n\n# Make some assignments\nC[1, 0] = 1\nB[0, 0] = 1\n\n# We can add C to each row of B as follows:\nA[i, j] =  B[i, j] + C[i, j]\n\n\n\n\nTaco currently does not support numpy-style broadcasting of singleton dimensions as evidenced by the snippet above. \n\n\nFunctional-style Interface\n\n\nTaco supports einsum notation through the interface \npytaco.einsum\n along with its own default parser available through \npytaco.parser\n.\n\n\nThe \neinsum\n function takes an einsum string as input along with a variable number of tensors. The einsum interface follows the following format:\n\n\npytaco.einsum(subscripts, *operands, out_format, dtype=float)\n\n\nThe \npytaco.parser\n string follows syntax exactly like the \nC++ API index notation\n. Notice that this differs from the python notation in that parenthesis are used instead of square brackets and we do not need to index scalars. The parser has the following spec:\n\n\npytaco.parser(string, *operands, out_format, dtype=float)\n\n\nIn addition, we also allow the following convenience functions:\n\n\nGeneral element-wise functions:\n\n\npt.add(a, b, out_format=pt.compressed, dtype=pt.float32)\n\n\npt.sub(a, b, out_format=pt.compressed, dtype=pt.float32)\n\n\npt.mul(a, b, out_format=pt.compressed, dtype=pt.float32)\n\n\npt.div(a, b, out_format=pt.compressed, dtype=pt.float32)\n\n\npt.floordiv(a, b, out_format=pt.compressed, dtype=pt.float32)\n\n\nMatrix multiply for order-2 tensors:\n\n\npt.matmul(a, b, out_format=pt.compressed, dtype=pt.float32)\n\n\nReduction functions for tensors along given axes. One can specify the axis they would like to reduce across or a list of axes to reduce:\n\n\npt.reduce_sum(a, axis=None, out_format=pt.dense, dtype=pt.float32)\n\n\npt.reduce_mul(a, axis=None, out_format=pt.dense, dtype=pt.float32)\n\n\nExamples:\n\n\nimport pytaco as pt\ni, j, k = pt.get_index_vars(3)\n\n# Make a compressed tensor of size 3x3\nA = pt.tensor([3,3])\nB = pt.tensor([3,3])\n\n# Make some assignments\nA[0, 0] = 9\nA[1, 0] = 1\nA[2, 0] = 2\nB[0, 0] = 1\n\n# Element-wise addition\nC = pt.add(A, B)\n\n# Element-wise multiplication\nD = pt.mul(A, B)\n\n# Matrix-multiplication\nE = pt.matmul(A, B)\n\n# Sum over all elements in A\nF = pt.reduce_sum(A)\n\n# Sum the columns of A\nG = pt.reduce_sum(A, axis=1)\n\n\n\n\nNumpy arrays can also be passed where tensors are expected in the functional interface.\n\n\nLimitations\n\n\nFor all forms of index expressions, we do not support indexing a tensor with the same index variable. For example expressions such as \nA[i,i]\n are disallowed.\n\n\nTransposes are not allowed during computations. The user will need to explicitly transpose a tensor themselves using \npt.tensor.transpose(new_ordering)\n before doing the computation.", 
            "title": "Computing on Tensors"
        }, 
        {
            "location": "/pycomputations/index.html#specifying-tensor-algebra-computations", 
            "text": "Tensor algebra computations can be expressed in taco with tensor index notation, which on a high level describes how each element in the output tensor can be computed from elements in the input tensors.   Tensor index notation  is described in detail in the description of the C++ library. Here, we focus on the syntax for python.  The syntax is very similar to C++ except that we use square brackets  []  to access tensors instead of parenthesis. For instance, to express  matrix addition  in Python, we simply need to write:  A[i, j] = B[i, j] + C[i, j]  where  A ,  B , and  C  denote order-2 tensors (i.e. matrices) while  i  and  j  are index variables that represent abstract indices into the corresponding dimensions of the tensors as in the C++ API.  It is important to note that due to the complications that arise from assembling sparse structures, we cannot have a tensor appear both on the left hand side and the right hand side of an expression.", 
            "title": "Specifying Tensor Algebra Computations"
        }, 
        {
            "location": "/pycomputations/index.html#indexvars", 
            "text": "As in the C++ API, index vars are needed to form index expressions. Tensors can be accessed with either a  pytaco.indexVar  to express computations or integers to read or write to a location in a tensor. So for instance, before specifying the above we could write:  import pytaco as pt\n\n# First method to make index vars\ni, j = pt.indexVar(), pt.indexVar()  We can also give names to indexVars as shown below:  import pytaco as pt\n\n# Named indexVars\ni, j = pt.indexVar( i ), pt.indexVar( j )  To make this less cumbersome, we also provide a function  pytaco.get_index_vars  to generate a list of n different index vars that can be used in tensor expressions.  import pytaco as pt\ni, j = pt.get_index_vars(2)  NOTE: When using scalars to express computations we must still use the square brackets to index the tensor. Since scalars are order-0 tensors,  None  must be passed into the index to specify that no  indexVar s are used:  import pytaco as pt\ni, j = pt.get_index_vars(2)\n\n# Make a scalar value\nA = pt.tensor(0)\n# Make a compressed tensor of size 3x3\nB = pt.tensor([3,3])\n\n# Make some assignments\nB[0, 0] = 1\nB[1, 0] = 10\n\n# We can sum the elements in B as follows. Notice we need to use None to tell \n# taco that A is a scalar.\nA[None] = B[i, j]", 
            "title": "IndexVars"
        }, 
        {
            "location": "/pycomputations/index.html#broadcasting", 
            "text": "When using  indexVar s, we must ensure that dimensions with the same  indexVar  are of the same size. Operations can be broadcast along outer dimensions assuming the inner dimensions are of the same size. For example:  import pytaco as pt\ni, j, k = pt.get_index_vars(3)\n\n# Make a compressed tensor of size 3x3\nA = pt.tensor([3,3])\nB = pt.tensor([3,3])\n\n# Make a dense vector\nC = pt.tensor([3], pt.dense)\n\n# Make some assignments\nC[0] = 1\nB[0, 0] = 1\n\n# We can add C to each row of B as follows:\nA[i, j] =  B[i, j] + C[j]  The following, however, is not valid since the dimension of index j is of a different size for the different tensors:  import pytaco as pt\ni, j, k = pt.get_index_vars(3)\n\n# Make a compressed tensor of size 3x3\nA = pt.tensor([3,3])\nB = pt.tensor([3,3])\n\n# Make a dense vector\nC = pt.tensor([3,1], pt.dense)\n\n# Make some assignments\nC[1, 0] = 1\nB[0, 0] = 1\n\n# We can add C to each row of B as follows:\nA[i, j] =  B[i, j] + C[i, j]  Taco currently does not support numpy-style broadcasting of singleton dimensions as evidenced by the snippet above.", 
            "title": "Broadcasting"
        }, 
        {
            "location": "/pycomputations/index.html#functional-style-interface", 
            "text": "Taco supports einsum notation through the interface  pytaco.einsum  along with its own default parser available through  pytaco.parser .  The  einsum  function takes an einsum string as input along with a variable number of tensors. The einsum interface follows the following format:  pytaco.einsum(subscripts, *operands, out_format, dtype=float)  The  pytaco.parser  string follows syntax exactly like the  C++ API index notation . Notice that this differs from the python notation in that parenthesis are used instead of square brackets and we do not need to index scalars. The parser has the following spec:  pytaco.parser(string, *operands, out_format, dtype=float)  In addition, we also allow the following convenience functions:  General element-wise functions:  pt.add(a, b, out_format=pt.compressed, dtype=pt.float32)  pt.sub(a, b, out_format=pt.compressed, dtype=pt.float32)  pt.mul(a, b, out_format=pt.compressed, dtype=pt.float32)  pt.div(a, b, out_format=pt.compressed, dtype=pt.float32)  pt.floordiv(a, b, out_format=pt.compressed, dtype=pt.float32)  Matrix multiply for order-2 tensors:  pt.matmul(a, b, out_format=pt.compressed, dtype=pt.float32)  Reduction functions for tensors along given axes. One can specify the axis they would like to reduce across or a list of axes to reduce:  pt.reduce_sum(a, axis=None, out_format=pt.dense, dtype=pt.float32)  pt.reduce_mul(a, axis=None, out_format=pt.dense, dtype=pt.float32)  Examples:  import pytaco as pt\ni, j, k = pt.get_index_vars(3)\n\n# Make a compressed tensor of size 3x3\nA = pt.tensor([3,3])\nB = pt.tensor([3,3])\n\n# Make some assignments\nA[0, 0] = 9\nA[1, 0] = 1\nA[2, 0] = 2\nB[0, 0] = 1\n\n# Element-wise addition\nC = pt.add(A, B)\n\n# Element-wise multiplication\nD = pt.mul(A, B)\n\n# Matrix-multiplication\nE = pt.matmul(A, B)\n\n# Sum over all elements in A\nF = pt.reduce_sum(A)\n\n# Sum the columns of A\nG = pt.reduce_sum(A, axis=1)  Numpy arrays can also be passed where tensors are expected in the functional interface.", 
            "title": "Functional-style Interface"
        }, 
        {
            "location": "/pycomputations/index.html#limitations", 
            "text": "For all forms of index expressions, we do not support indexing a tensor with the same index variable. For example expressions such as  A[i,i]  are disallowed.  Transposes are not allowed during computations. The user will need to explicitly transpose a tensor themselves using  pt.tensor.transpose(new_ordering)  before doing the computation.", 
            "title": "Limitations"
        }, 
        {
            "location": "/tensors/index.html", 
            "text": "Declaring Tensors\n\n\ntaco::Tensor\n objects, which correspond to mathematical tensors, form the core of the taco C++ library. You can declare a new tensor by specifying its name, a vector containing the size of each dimension of the tensor, and the \nstorage format\n that will be used to store the tensor:\n\n\n// Declare a new tensor \nA\n of double-precision floats with dimensions \n// 512 x 64 x 2048, stored as a dense-sparse-sparse tensor\nTensor\ndouble\n A(\nA\n, {512,64,2048}, Format({Dense,Sparse,Sparse}));\n\n\n\n\nThe name of the tensor can be omitted, in which case taco will assign an arbitrary name to the tensor:\n\n\n// Declare another tensor with the same dimensions and storage format as before\nTensor\ndouble\n A({512,64,2048}, Format({Dense,Sparse,Sparse}));\n\n\n\n\nScalars, which are treated as order-0 tensors, can be declared and initialized with some arbitrary value as demonstrated below:\n\n\nTensor\ndouble\n alpha(42.0);  // Declare a scalar tensor initialized to 42.0\n\n\n\n\nDefining Tensor Formats\n\n\nConceptually, you can think of a tensor as a tree with each level (excluding the root) corresponding to a dimension of the tensor. Each path from the root to a leaf node represents a tensor coordinate and its corresponding value. Which dimension each level of the tree corresponds to is determined by the order in which dimensions of the tensor are stored.\n\n\ntaco uses a novel scheme that can describe different storage formats for any tensor by specifying the order in which tensor dimensions are stored and whether each dimension is sparse or dense. A sparse dimension stores only the subset of the dimension that contains non-zero values and is conceptually similar to the index arrays used in the compressed sparse row (CSR) matrix format, while a dense dimension stores both zeros and non-zeros. As demonstrated below, this scheme is flexibile enough to express many commonly-used matrix storage formats.\n\n\nYou can define a new tensor storage format by creating a \ntaco::Format\n object. The constructor for \ntaco::Format\n takes as arguments a vector specifying the type of each dimension and (optionally) a vector specifying the order in which dimensions are to be stored, following the above scheme:\n\n\nFormat   dm({Dense,Dense});           // (Row-major) dense matrix\nFormat  csr({Dense,Sparse});          // Compressed sparse row matrix\nFormat  csc({Dense,Sparse}, {1,0});   // Compressed sparse column matrix\nFormat dcsr({Sparse,Sparse}, {1,0});  // Doubly compressed sparse column matrix\n\n\n\n\nAlternatively, you can define a tensor format that contains only sparse or dense dimensions as follows:\n\n\nFormat csf(Sparse);  // Compressed sparse fiber tensor\n\n\n\n\nInitializing Tensors\n\n\nYou can initialize a \ntaco::Tensor\n by calling the \ninsert\n method to add a non-zero component to the tensor. The \ninsert\n method takes two arguments, a vector specifying the coordinate of the non-zero component to be added and the value to be inserted at that coordinate:\n\n\nA.insert({128,32,1024}, 42.0);  // A(128,32,1024) = 42.0\n\n\n\n\nThe \ninsert\n method adds the inserted non-zeros to a temporary buffer. Before a tensor can actually be used in a computation though, you must invoke the \npack\n method to compress the tensor into the storage format that was specified when the tensor was first declared:\n\n\nA.pack();  // Construct dense-sparse-sparse tensor containing inserted non-zeros\n\n\n\n\nLoading Tensors from File\n\n\nRather than manually invoking \ninsert\n and \npack\n to initialize a tensor, you can load tensors directly from file by calling \ntaco::read\n as demonstrated below:\n\n\n// Load a dense-sparse-sparse tensor from file A.tns\nA = read(\nA.tns\n, Format({Dense, Sparse, Sparse}));\n\n\n\n\nBy default, \ntaco::read\n returns a packed tensor. You can optionally pass a Boolean flag as an argument to indicate whether the returned tensor should be packed or not:\n\n\n// Load an unpacked tensor from file A.tns\nA = read(\nA.tns\n, Format({Dense, Sparse, Sparse}), false);\n\n\n\n\nCurrently, taco supports loading from the following matrix and tensor file formats:\n\n\n\n\nMatrix Market (Coordinate) Format (.mtx)\n\n\nRutherford-Boeing Format (.rb)\n\n\nFROSTT Format (.tns)\n\n\n\n\nWriting Tensors to File\n\n\nYou can also write a (packed) tensor directly to file by calling \ntaco::write\n, as demonstrated below:\n\n\nwrite(\nA.tns\n, A);  // Write tensor A to file A.tns\n\n\n\n\ntaco::write\n supports the same set of matrix and tensor file formats as \ntaco::read\n.", 
            "title": "Defining Tensors"
        }, 
        {
            "location": "/tensors/index.html#declaring-tensors", 
            "text": "taco::Tensor  objects, which correspond to mathematical tensors, form the core of the taco C++ library. You can declare a new tensor by specifying its name, a vector containing the size of each dimension of the tensor, and the  storage format  that will be used to store the tensor:  // Declare a new tensor  A  of double-precision floats with dimensions \n// 512 x 64 x 2048, stored as a dense-sparse-sparse tensor\nTensor double  A( A , {512,64,2048}, Format({Dense,Sparse,Sparse}));  The name of the tensor can be omitted, in which case taco will assign an arbitrary name to the tensor:  // Declare another tensor with the same dimensions and storage format as before\nTensor double  A({512,64,2048}, Format({Dense,Sparse,Sparse}));  Scalars, which are treated as order-0 tensors, can be declared and initialized with some arbitrary value as demonstrated below:  Tensor double  alpha(42.0);  // Declare a scalar tensor initialized to 42.0", 
            "title": "Declaring Tensors"
        }, 
        {
            "location": "/tensors/index.html#defining-tensor-formats", 
            "text": "Conceptually, you can think of a tensor as a tree with each level (excluding the root) corresponding to a dimension of the tensor. Each path from the root to a leaf node represents a tensor coordinate and its corresponding value. Which dimension each level of the tree corresponds to is determined by the order in which dimensions of the tensor are stored.  taco uses a novel scheme that can describe different storage formats for any tensor by specifying the order in which tensor dimensions are stored and whether each dimension is sparse or dense. A sparse dimension stores only the subset of the dimension that contains non-zero values and is conceptually similar to the index arrays used in the compressed sparse row (CSR) matrix format, while a dense dimension stores both zeros and non-zeros. As demonstrated below, this scheme is flexibile enough to express many commonly-used matrix storage formats.  You can define a new tensor storage format by creating a  taco::Format  object. The constructor for  taco::Format  takes as arguments a vector specifying the type of each dimension and (optionally) a vector specifying the order in which dimensions are to be stored, following the above scheme:  Format   dm({Dense,Dense});           // (Row-major) dense matrix\nFormat  csr({Dense,Sparse});          // Compressed sparse row matrix\nFormat  csc({Dense,Sparse}, {1,0});   // Compressed sparse column matrix\nFormat dcsr({Sparse,Sparse}, {1,0});  // Doubly compressed sparse column matrix  Alternatively, you can define a tensor format that contains only sparse or dense dimensions as follows:  Format csf(Sparse);  // Compressed sparse fiber tensor", 
            "title": "Defining Tensor Formats"
        }, 
        {
            "location": "/tensors/index.html#initializing-tensors", 
            "text": "You can initialize a  taco::Tensor  by calling the  insert  method to add a non-zero component to the tensor. The  insert  method takes two arguments, a vector specifying the coordinate of the non-zero component to be added and the value to be inserted at that coordinate:  A.insert({128,32,1024}, 42.0);  // A(128,32,1024) = 42.0  The  insert  method adds the inserted non-zeros to a temporary buffer. Before a tensor can actually be used in a computation though, you must invoke the  pack  method to compress the tensor into the storage format that was specified when the tensor was first declared:  A.pack();  // Construct dense-sparse-sparse tensor containing inserted non-zeros", 
            "title": "Initializing Tensors"
        }, 
        {
            "location": "/tensors/index.html#loading-tensors-from-file", 
            "text": "Rather than manually invoking  insert  and  pack  to initialize a tensor, you can load tensors directly from file by calling  taco::read  as demonstrated below:  // Load a dense-sparse-sparse tensor from file A.tns\nA = read( A.tns , Format({Dense, Sparse, Sparse}));  By default,  taco::read  returns a packed tensor. You can optionally pass a Boolean flag as an argument to indicate whether the returned tensor should be packed or not:  // Load an unpacked tensor from file A.tns\nA = read( A.tns , Format({Dense, Sparse, Sparse}), false);  Currently, taco supports loading from the following matrix and tensor file formats:   Matrix Market (Coordinate) Format (.mtx)  Rutherford-Boeing Format (.rb)  FROSTT Format (.tns)", 
            "title": "Loading Tensors from File"
        }, 
        {
            "location": "/tensors/index.html#writing-tensors-to-file", 
            "text": "You can also write a (packed) tensor directly to file by calling  taco::write , as demonstrated below:  write( A.tns , A);  // Write tensor A to file A.tns  taco::write  supports the same set of matrix and tensor file formats as  taco::read .", 
            "title": "Writing Tensors to File"
        }, 
        {
            "location": "/computations/index.html", 
            "text": "Specifying Tensor Algebra Computations\n\n\nTensor algebra computations can be expressed in taco with tensor index notation, which on a high level describes how each element in the output tensor can be computed from elements in the input tensors. As an example, matrix addition can be expressed in index notation as \n\n\nA(i,j) = B(i,j) + C(i,j)\n\n\n\n\nwhere \nA\n, \nB\n, and \nC\n denote order-2 tensors (i.e. matrices) while \ni\n and \nj\n are index variables that represent abstract indices into the corresponding dimensions of the tensors. In words, the example above essentially states that, for every \ni\n and \nj\n, the element in the \ni\n-th row and \nj\n-th column of the \nA\n should be assigned the sum of the corresponding elements in \nB\n and \nC\n. Similarly, element-wise multiplication of three order-3 tensors can be expressed in index notation as follows\n\n\nA(i,j,k) = B(i,j,k) * C(i,j,k) * D(i,j,k)\n\n\n\n\nThe syntax shown above corresponds to exactly what you would have to write in C++ with the taco library to define tensor algebra computations. Note, however, that prior to defining a tensor algebra computation, all index variables have to be declared. This can be done as shown below:\n\n\nIndexVar i, j, k;  // Declare index variables for previous example\n\n\n\n\nExpressing Reductions\n\n\nIn both of the previous examples, all of the index variables are used to index into both the output and the inputs. However, it is possible for an index variable to be used to index into the inputs only, in which case the index variable is reduced (summed) over. For instance, the following example \n\n\ny(i) = A(i,j) * x(j)\n\n\n\n\ncan be rewritten with the summation more explicit as \ny(i) = \\sum_{j} A(i,j) \\cdot x(j)\n and demonstrates how matrix-vector multiplication can be expressed in index notation.\n\n\nNote that, in taco, reductions are assumed to be over the smallest subexpression that captures all uses of the corresponding reduction variable. For instance, the following computation \n\n\ny(i) = A(i,j) * x(j) + z(i)\n\n\n\n\ncan be rewritten with the summation more explicit as \n\n\n\n\ny(i) = \\big(\\sum_{j} A(i,j) \\cdot x(j)\\big) + z(i),\n\n\n\n\nwhereas the following computation \n\n\ny(i) = A(i,j) * x(j) + z(j)\n\n\n\n\ncan be rewritten with the summation more explicit as \n\n\n\n\ny(i) = \\sum_{j} \\big(A(i,j) \\cdot x(j) + z(i)\\big).\n\n\n\n\nPerforming the Computation\n\n\nOnce a tensor algebra computation has been defined (and all of the inputs have been \ninitialized\n), you can simply invoke the output tensor's \nevaluate\n method to perform the actual computation:\n\n\nA.evaluate();  // Perform the computation defined previously for output tensor A\n\n\n\n\nUnder the hood, when you invoke the \nevaluate\n method, taco first invokes the output tensor's \ncompile\n method to generate kernels that assembles the output indices (if the tensor contains any sparse dimensions) and that performs the actual computation. taco would then call the two generated kernels by invoking the output tensor's \nassemble\n and \ncompute\n methods. You can manually invoke these methods instead of calling \nevaluate\n as demonstrated below:\n\n\nA.compile();   // Generate output assembly and compute kernels \nA.assemble();  // Invoke the output assembly kernel to assemble the output indices\nA.compute();   // Invoke the compute kernel to perform the actual computation\n\n\n\n\nThis can be useful if you want to perform the same computation multiple times, in which case it suffices to invoke \ncompile\n once before the first time the computation is performed.\n\n\nDelayed Execuation\n\n\nIt is also possible to skip using the compiler functions entirely. Once you attempt to modify or view the output tensor, taco will automatically invoke the compiler in order to generate the data. \n\n\nIt should be noted that in order to accurately time a computation, it is necessary to invoke the compiler functions directly since relying on the delayed execution mechanism can cause a lot of prior computations to be included in the timing.", 
            "title": "Computing on Tensors"
        }, 
        {
            "location": "/computations/index.html#specifying-tensor-algebra-computations", 
            "text": "Tensor algebra computations can be expressed in taco with tensor index notation, which on a high level describes how each element in the output tensor can be computed from elements in the input tensors. As an example, matrix addition can be expressed in index notation as   A(i,j) = B(i,j) + C(i,j)  where  A ,  B , and  C  denote order-2 tensors (i.e. matrices) while  i  and  j  are index variables that represent abstract indices into the corresponding dimensions of the tensors. In words, the example above essentially states that, for every  i  and  j , the element in the  i -th row and  j -th column of the  A  should be assigned the sum of the corresponding elements in  B  and  C . Similarly, element-wise multiplication of three order-3 tensors can be expressed in index notation as follows  A(i,j,k) = B(i,j,k) * C(i,j,k) * D(i,j,k)  The syntax shown above corresponds to exactly what you would have to write in C++ with the taco library to define tensor algebra computations. Note, however, that prior to defining a tensor algebra computation, all index variables have to be declared. This can be done as shown below:  IndexVar i, j, k;  // Declare index variables for previous example", 
            "title": "Specifying Tensor Algebra Computations"
        }, 
        {
            "location": "/computations/index.html#expressing-reductions", 
            "text": "In both of the previous examples, all of the index variables are used to index into both the output and the inputs. However, it is possible for an index variable to be used to index into the inputs only, in which case the index variable is reduced (summed) over. For instance, the following example   y(i) = A(i,j) * x(j)  can be rewritten with the summation more explicit as  y(i) = \\sum_{j} A(i,j) \\cdot x(j)  and demonstrates how matrix-vector multiplication can be expressed in index notation.  Note that, in taco, reductions are assumed to be over the smallest subexpression that captures all uses of the corresponding reduction variable. For instance, the following computation   y(i) = A(i,j) * x(j) + z(i)  can be rewritten with the summation more explicit as    y(i) = \\big(\\sum_{j} A(i,j) \\cdot x(j)\\big) + z(i),   whereas the following computation   y(i) = A(i,j) * x(j) + z(j)  can be rewritten with the summation more explicit as    y(i) = \\sum_{j} \\big(A(i,j) \\cdot x(j) + z(i)\\big).", 
            "title": "Expressing Reductions"
        }, 
        {
            "location": "/computations/index.html#performing-the-computation", 
            "text": "Once a tensor algebra computation has been defined (and all of the inputs have been  initialized ), you can simply invoke the output tensor's  evaluate  method to perform the actual computation:  A.evaluate();  // Perform the computation defined previously for output tensor A  Under the hood, when you invoke the  evaluate  method, taco first invokes the output tensor's  compile  method to generate kernels that assembles the output indices (if the tensor contains any sparse dimensions) and that performs the actual computation. taco would then call the two generated kernels by invoking the output tensor's  assemble  and  compute  methods. You can manually invoke these methods instead of calling  evaluate  as demonstrated below:  A.compile();   // Generate output assembly and compute kernels \nA.assemble();  // Invoke the output assembly kernel to assemble the output indices\nA.compute();   // Invoke the compute kernel to perform the actual computation  This can be useful if you want to perform the same computation multiple times, in which case it suffices to invoke  compile  once before the first time the computation is performed.", 
            "title": "Performing the Computation"
        }, 
        {
            "location": "/computations/index.html#delayed-execuation", 
            "text": "It is also possible to skip using the compiler functions entirely. Once you attempt to modify or view the output tensor, taco will automatically invoke the compiler in order to generate the data.   It should be noted that in order to accurately time a computation, it is necessary to invoke the compiler functions directly since relying on the delayed execution mechanism can cause a lot of prior computations to be included in the timing.", 
            "title": "Delayed Execuation"
        }, 
        {
            "location": "/scientific_computing/index.html", 
            "text": "Sparse matrix-vector multiplication (SpMV) is a bottleneck operation in many scientific and engineering computations. Mathematically, the operation demonstrated in this example can be expressed as \ny = \\alpha Ax + \\beta z\n, where \nx\n, \ny\n, and \nz\n are dense vectors, \nA\n is a sparse matrix, and \n\\alpha\n and \n\\beta\n are scalar values. This operation can also be expressed in \nindex notation\n as \n\n\ny(i) = alpha * A(i,j) * x(j) + beta * z(i)\n\n\n\n\nYou can use the taco Python library to easily and efficiently compute the SpMV as demonstrated here:\n\n\nimport pytaco as pt\nfrom pytaco import compressed, dense\nimport numpy as np\n\n# Declare the storage formats as explained in the C++ sample\ncsr = pt.format([dense, compressed])\ndv  = pt.format([dense])\n\n# Load a sparse matrix stored in the matrix market format) and store it as a csr matrix. \n# The matrix  # in this example can be downloaded from:\n# https://www.cise.ufl.edu/research/sparse/MM/Boeing/pwtk.tar.gz\nA = pt.read(\npwtk.mtx\n, csr)\n\n# Generate two random vectors using numpy and pass them into taco\nx = pt.from_numpy_array(np.random.uniform(size=A.shape[0]))\nz = pt.from_numpy_array(np.random.uniform(size=A.shape[0]))\n\n# Declare output vector as dense\ny = pt.tensor([A.shape[0]], dv)\n\n# Create index vars\ni, j = pt.get_index_vars(2)\n\n# Define the SpMV computation\ny[i] = 42 * A[i, j] * x[j] + 33 * z[i]\n\n# Store the output\npt.write(\ny.tns\n, y)\n\n\n\n\nUnder the hood, when you run the above Python program, taco generates the imperative code shown below to compute the SpMV. taco is able to evaluate this compound operation efficiently with a single kernel that avoids materializing the intermediate matrix-vector product.\n\n\nfor (int iA = 0; iA \n 217918; iA++) {\n  double tj = 0;\n  for (int A2_pos = A.d2.pos[iA]; A2_pos \n A.d2.pos[(iA + 1)]; A2_pos++) {\n    int jA = A.d2.idx[A2_pos];\n    tj += A.vals[A2_pos] * x.vals[jA];\n  }\n  y.vals[iA] = (alpha.vals[0] * tj) + (beta.vals[0] * z.vals[iA]);\n}", 
            "title": "Scientific Computing: SpMV"
        }, 
        {
            "location": "/machine_learning/index.html", 
            "text": "Sampled dense-dense matrix product (SDDMM) is a bottleneck operation in many factor analysis algorithms used in machine learning, including Alternating \nLeast Squares and Latent Dirichlet Allocation [1]. Mathematically, the operation can be expressed as \nA = B \\circ CD\n, where \nA\n and \nB\n are sparse matrices, \nC\n and \nD\n are dense matrices, and \n\\circ\n denotes component-wise multiplication. This operation can also be expressed in \nindex notation\n as \n\n\nA(i,j) = B(i,j) * C(i,k) * D(k,j)\n\n\n\n\nYou can use the taco Python library to easily and efficiently compute the SDDMM as demonstrated here:\n\n\nimport pytaco as pt\nfrom pytaco import dense, compressed, format\nimport numpy as np\n\n# Predeclare the storage formats that the inputs and output will be stored as.\n# To define a format, you must specify whether each dimension is dense or sparse \n# and (optionally) the order in which dimensions should be stored. The formats \n# declared below correspond to doubly compressed sparse row (dcsr), row-major \n# dense (rm), and column-major dense (dm).\ndcsr = format([compressed, compressed])\nrm   = format([dense, dense])\ncm   = format([dense, dense], [1, 0])\n\n# The matrix in this example can be download from:\n# https://www.cise.ufl.edu/research/sparse/MM/Williams/webbase-1M.tar.gz\nB = pt.read(\nwebbase-1M.mtx\n, dcsr)\n\n# Use numpy to create random matrices\nx = pt.from_numpy_array(np.random.uniform( size=(B.shape[0], 1000) ) )\nz = pt.from_numpy_array(np.random.uniform( size=(1000, B.shape[1]) ), out_format=cm )\n\n# Declare output matrix as doubly compressed sparse row\nA = pt.tensor(B.shape, dcsr)\n\n# Create index vars\ni, j, k = pt.get_index_vars(3)\nA[i, j] = B[i, j] * C[i, k] * D[k, j]\n\n# store tensor\npt.write(\nA.mtx\n, A)\n\n\n\n\nUnder the hood, when you run the above Python program, taco generates the imperative code shown below to compute the SDDMM. taco is able to do this efficiently by only computing entries of the intermediate matrix product that are actually needed to compute the output tensor \nA\n.\n\n\nint A1_pos = A.d1.pos[0];\nint A2_pos = A.d2.pos[A1_pos];\nfor (int B1_pos = B.d1.pos[0]; B1_pos \n B.d1.pos[(0 + 1)]; B1_pos++) {\n  int iB = B.d1.idx[B1_pos];\n  for (int B2_pos = B.d2.pos[B1_pos]; B2_pos \n B.d2.pos[(B1_pos + 1)]; B2_pos++) {\n    int jB = B.d2.idx[B2_pos];\n    double tk = 0;\n    for (int kC = 0; kC \n 1000; kC++) {\n      int C2_pos = (iB * 1000) + kC;\n      int D2_pos = (jB * 1000) + kC;\n      tk += (B.vals[B2_pos] * C.vals[C2_pos]) * D.vals[D2_pos];\n    }\n    A.vals[A2_pos] = tk;\n    A2_pos++;\n  }\n  if (A.d2.pos[(A1_pos + 1)] \n A.d2.pos[A1_pos]) A1_pos++;\n}\n\n\n\n\n[1] Huasha Zhao. 2014. High Performance Machine Learning through Codesign and Rooflining. Ph.D. Dissertation. EECS Department, University of California, Berkeley.", 
            "title": "Machine Learning: SDDMM"
        }, 
        {
            "location": "/data_analytics/index.html", 
            "text": "Matricized tensor times Khatri-Rao product (MTTKRP) is a bottleneck operation in various algorithms - such as Alternating Least Squares - for computing sparse tensor factorizations like the Canonical Polyadic Decomposition. Mathematically, mode-1 MTTKRP (for order-3 tensors) can be expressed as \nA = B_{(1)} (D \\odot C)\n, where \nA\n, \nC\n, and \nD\n are (typically) dense matrices, \nB\n is an order-3 tensor (matricizied along the first mode), and \n\\odot\n denotes the Khatri-Rao product. This operation can also be expressed in \nindex notation\n as \n\n\nA(i,j) = B(i,k,l) * D(l,j) * C(k,j)\n\n\n\n\nYou can use the taco Python library to easily and efficiently compute the MTTKRP as demonstrated here:\n\n\nimport pytaco as pt\nimport numpy as np\nfrom pytaco import compressed, dense, format\n\n# Declare tensor formats\ncsf = format([compressed, compressed, compressed])\nrm  = format([dense, dense])\n\n# Load a sparse order-3 tensor from file (stored in the FROSTT format) and \n# store it as a compressed sparse fiber tensor. The tensor in this example \n# can be download from: http://frostt.io/tensors/nell-2/\nB = pt.read(\nnell-2.tns\n, csf);\n\n# Use numpy to create random matrices\nC = pt.from_numpy_array(np.random.uniform( size=(B.shape[1], 25) ) )\nD = pt.from_numpy_array(np.random.uniform( size=(B.shape[2], 25) ) )\n\n# Create output tensor\nA = pt.tensor([B.shape[0], 25], rm)\n\n# Create index vars and define the MTTKRP op\ni, j, k, l = get_index_vars(4)\nA[i, j] = B[i, k, l] * D[l, j] * C[k, j]\n\npt.write(\nA.tns\n, A)\n\n\n\n\nUnder the hood, when you run the above Python program, taco generates the imperative code shown below to compute the MTTKRP. taco is able to evaluate this compound operation efficiently with a single kernel that avoids materializing the intermediate Khatri-Rao product.\n\n\nfor (int B1_pos = B.d1.pos[0]; B1_pos \n B.d1.pos[(0 + 1)]; B1_pos++) {\n  int iB = B.d1.idx[B1_pos];\n  for (int B2_pos = B.d2.pos[B1_pos]; B2_pos \n B.d2.pos[(B1_pos + 1)]; B2_pos++) {\n    int kB = B.d2.idx[B2_pos];\n    for (int B3_pos = B.d3.pos[B2_pos]; B3_pos \n B.d3.pos[(B2_pos + 1)]; B3_pos++) {\n      int lB = B.d3.idx[B3_pos];\n      double t37 = B.vals[B3_pos];\n      for (int jD = 0; jD \n 25; jD++) {\n        int D2_pos = (lB * 25) + jD;\n        int C2_pos = (kB * 25) + jD;\n        int A2_pos = (iB * 25) + jD;\n        A.vals[A2_pos] = A.vals[A2_pos] + ((t37 * D.vals[D2_pos]) * C.vals[C2_pos]);\n      }\n    }\n  }\n}", 
            "title": "Data Analytics: MTTKRP"
        }
    ]
}