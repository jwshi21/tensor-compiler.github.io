{
    "docs": [
        {
            "location": "/index.html", 
            "text": "TACO\n is a library for performing sparse and\ndense linear algebra and tensor algebra computations. The computations can\nrange from relatively simple ones like sparse matrix-vector multiplication to\nmore complex ones like matricized tensor times Khatri-Rao product.  All these\ncomputations can be performed on any mix of dense and sparse tensors. Under the\nhood, TACO automatically generates efficient code to perform these\ncomputations.\n\n\nThe sidebar to the left links to documentation for the TACO C++ and Python\nlibraries as well as some examples demonstrating how TACO can be used in\nreal-world applications.\n\n\nSystem Requirements\n\n\n\n\nA C compiler that supports C99, such as GCC or Clang\n\n\nSupport for OpenMP is also required if parallel execution is desired\n\n\n\n\n\n\nPython 3 with NumPy and SciPy (for the Python library)\n\n\n\n\nGetting Help\n\n\nQuestions and bug reports can be submitted \nhere\n.", 
            "title": "Home"
        }, 
        {
            "location": "/index.html#system-requirements", 
            "text": "A C compiler that supports C99, such as GCC or Clang  Support for OpenMP is also required if parallel execution is desired    Python 3 with NumPy and SciPy (for the Python library)", 
            "title": "System Requirements"
        }, 
        {
            "location": "/index.html#getting-help", 
            "text": "Questions and bug reports can be submitted  here .", 
            "title": "Getting Help"
        }, 
        {
            "location": "/tensors/index.html", 
            "text": "Declaring Tensors\n\n\ntaco::Tensor\n objects, which correspond to mathematical tensors, form the core of the taco C++ library. You can declare a new tensor by specifying its name, a vector containing the size of each dimension of the tensor, and the \nstorage format\n that will be used to store the tensor:\n\n\n// Declare a new tensor \"A\" of double-precision floats with dimensions \n// 512 x 64 x 2048, stored as a dense-sparse-sparse tensor\nTensor\ndouble\n A(\"A\", {512,64,2048}, Format({Dense,Sparse,Sparse}));\n\n\n\nThe name of the tensor can be omitted, in which case taco will assign an arbitrary name to the tensor:\n\n\n// Declare another tensor with the same dimensions and storage format as before\nTensor\ndouble\n A({512,64,2048}, Format({Dense,Sparse,Sparse}));\n\n\n\nScalars, which are treated as order-0 tensors, can be declared and initialized with some arbitrary value as demonstrated below:\n\n\nTensor\ndouble\n alpha(42.0);  // Declare a scalar tensor initialized to 42.0\n\n\n\nDefining Tensor Formats\n\n\nConceptually, you can think of a tensor as a tree with each level (excluding the root) corresponding to a dimension of the tensor. Each path from the root to a leaf node represents a tensor coordinate and its corresponding value. Which dimension each level of the tree corresponds to is determined by the order in which dimensions of the tensor are stored.\n\n\ntaco uses a novel scheme that can describe different storage formats for any tensor by specifying the order in which tensor dimensions are stored and whether each dimension is sparse or dense. A sparse dimension stores only the subset of the dimension that contains non-zero values and is conceptually similar to the index arrays used in the compressed sparse row (CSR) matrix format, while a dense dimension stores both zeros and non-zeros. As demonstrated below, this scheme is flexibile enough to express many commonly-used matrix storage formats.\n\n\nYou can define a new tensor storage format by creating a \ntaco::Format\n object. The constructor for \ntaco::Format\n takes as arguments a vector specifying the type of each dimension and (optionally) a vector specifying the order in which dimensions are to be stored, following the above scheme:\n\nFormat   dm({Dense,Dense});           // (Row-major) dense matrix\nFormat  csr({Dense,Sparse});          // Compressed sparse row matrix\nFormat  csc({Dense,Sparse}, {1,0});   // Compressed sparse column matrix\nFormat dcsr({Sparse,Sparse}, {1,0});  // Doubly compressed sparse column matrix\n\n\nAlternatively, you can define a tensor format that contains only sparse or dense dimensions as follows:\n\n\nFormat csf(Sparse);  // Compressed sparse fiber tensor\n\n\n\nInitializing Tensors\n\n\nYou can initialize a \ntaco::Tensor\n by calling the \ninsert\n method to add a non-zero component to the tensor. The \ninsert\n method takes two arguments, a vector specifying the coordinate of the non-zero component to be added and the value to be inserted at that coordinate:\n\n\nA.insert({128,32,1024}, 42.0);  // A(128,32,1024) = 42.0\n\n\n\nThe \ninsert\n method adds the inserted non-zeros to a temporary buffer. Before a tensor can actually be used in a computation though, you must invoke the \npack\n method to compress the tensor into the storage format that was specified when the tensor was first declared:\n\n\nA.pack();  // Construct dense-sparse-sparse tensor containing inserted non-zeros\n\n\n\nLoading Tensors from File\n\n\nRather than manually invoking \ninsert\n and \npack\n to initialize a tensor, you can load tensors directly from file by calling \ntaco::read\n as demonstrated below:\n\n\n// Load a dense-sparse-sparse tensor from file A.tns\nA = read(\"A.tns\", Format({Dense, Sparse, Sparse}));\n\n\n\nBy default, \ntaco::read\n returns a packed tensor. You can optionally pass a Boolean flag as an argument to indicate whether the returned tensor should be packed or not:\n\n\n// Load an unpacked tensor from file A.tns\nA = read(\"A.tns\", Format({Dense, Sparse, Sparse}), false);\n\n\n\nCurrently, taco supports loading from the following matrix and tensor file formats:\n\n\n\n\nMatrix Market (Coordinate) Format (.mtx)\n\n\nRutherford-Boeing Format (.rb)\n\n\nFROSTT Format (.tns)\n\n\n\n\nWriting Tensors to File\n\n\nYou can also write a (packed) tensor directly to file by calling \ntaco::write\n, as demonstrated below:\n\n\nwrite(\"A.tns\", A);  // Write tensor A to file A.tns\n\n\n\ntaco::write\n supports the same set of matrix and tensor file formats as \ntaco::read\n.", 
            "title": "Defining Tensors"
        }, 
        {
            "location": "/tensors/index.html#declaring-tensors", 
            "text": "taco::Tensor  objects, which correspond to mathematical tensors, form the core of the taco C++ library. You can declare a new tensor by specifying its name, a vector containing the size of each dimension of the tensor, and the  storage format  that will be used to store the tensor:  // Declare a new tensor \"A\" of double-precision floats with dimensions \n// 512 x 64 x 2048, stored as a dense-sparse-sparse tensor\nTensor double  A(\"A\", {512,64,2048}, Format({Dense,Sparse,Sparse}));  The name of the tensor can be omitted, in which case taco will assign an arbitrary name to the tensor:  // Declare another tensor with the same dimensions and storage format as before\nTensor double  A({512,64,2048}, Format({Dense,Sparse,Sparse}));  Scalars, which are treated as order-0 tensors, can be declared and initialized with some arbitrary value as demonstrated below:  Tensor double  alpha(42.0);  // Declare a scalar tensor initialized to 42.0", 
            "title": "Declaring Tensors"
        }, 
        {
            "location": "/tensors/index.html#defining-tensor-formats", 
            "text": "Conceptually, you can think of a tensor as a tree with each level (excluding the root) corresponding to a dimension of the tensor. Each path from the root to a leaf node represents a tensor coordinate and its corresponding value. Which dimension each level of the tree corresponds to is determined by the order in which dimensions of the tensor are stored.  taco uses a novel scheme that can describe different storage formats for any tensor by specifying the order in which tensor dimensions are stored and whether each dimension is sparse or dense. A sparse dimension stores only the subset of the dimension that contains non-zero values and is conceptually similar to the index arrays used in the compressed sparse row (CSR) matrix format, while a dense dimension stores both zeros and non-zeros. As demonstrated below, this scheme is flexibile enough to express many commonly-used matrix storage formats.  You can define a new tensor storage format by creating a  taco::Format  object. The constructor for  taco::Format  takes as arguments a vector specifying the type of each dimension and (optionally) a vector specifying the order in which dimensions are to be stored, following the above scheme: Format   dm({Dense,Dense});           // (Row-major) dense matrix\nFormat  csr({Dense,Sparse});          // Compressed sparse row matrix\nFormat  csc({Dense,Sparse}, {1,0});   // Compressed sparse column matrix\nFormat dcsr({Sparse,Sparse}, {1,0});  // Doubly compressed sparse column matrix  Alternatively, you can define a tensor format that contains only sparse or dense dimensions as follows:  Format csf(Sparse);  // Compressed sparse fiber tensor", 
            "title": "Defining Tensor Formats"
        }, 
        {
            "location": "/tensors/index.html#initializing-tensors", 
            "text": "You can initialize a  taco::Tensor  by calling the  insert  method to add a non-zero component to the tensor. The  insert  method takes two arguments, a vector specifying the coordinate of the non-zero component to be added and the value to be inserted at that coordinate:  A.insert({128,32,1024}, 42.0);  // A(128,32,1024) = 42.0  The  insert  method adds the inserted non-zeros to a temporary buffer. Before a tensor can actually be used in a computation though, you must invoke the  pack  method to compress the tensor into the storage format that was specified when the tensor was first declared:  A.pack();  // Construct dense-sparse-sparse tensor containing inserted non-zeros", 
            "title": "Initializing Tensors"
        }, 
        {
            "location": "/tensors/index.html#loading-tensors-from-file", 
            "text": "Rather than manually invoking  insert  and  pack  to initialize a tensor, you can load tensors directly from file by calling  taco::read  as demonstrated below:  // Load a dense-sparse-sparse tensor from file A.tns\nA = read(\"A.tns\", Format({Dense, Sparse, Sparse}));  By default,  taco::read  returns a packed tensor. You can optionally pass a Boolean flag as an argument to indicate whether the returned tensor should be packed or not:  // Load an unpacked tensor from file A.tns\nA = read(\"A.tns\", Format({Dense, Sparse, Sparse}), false);  Currently, taco supports loading from the following matrix and tensor file formats:   Matrix Market (Coordinate) Format (.mtx)  Rutherford-Boeing Format (.rb)  FROSTT Format (.tns)", 
            "title": "Loading Tensors from File"
        }, 
        {
            "location": "/tensors/index.html#writing-tensors-to-file", 
            "text": "You can also write a (packed) tensor directly to file by calling  taco::write , as demonstrated below:  write(\"A.tns\", A);  // Write tensor A to file A.tns  taco::write  supports the same set of matrix and tensor file formats as  taco::read .", 
            "title": "Writing Tensors to File"
        }, 
        {
            "location": "/computations/index.html", 
            "text": "Specifying Tensor Algebra Computations\n\n\nTensor algebra computations can be expressed in taco with tensor index notation, which at a high level describes how each element in the output tensor can be computed from elements in the input tensors. As an example, matrix addition can be expressed in index notation as \n\n\nA(i,j) = B(i,j) + C(i,j)\n\n\n\nwhere \nA\n, \nB\n, and \nC\n denote order-2 tensors (i.e. matrices) while \ni\n and \nj\n are index variables that represent abstract indices into the corresponding dimensions of the tensors. In words, the example above essentially states that, for every \ni\n and \nj\n, the element in the \ni\n-th row and \nj\n-th column of the \nA\n should be assigned the sum of the corresponding elements in \nB\n and \nC\n. Similarly, element-wise multiplication of three order-3 tensors can be expressed in index notation as follows\n\n\nA(i,j,k) = B(i,j,k) * C(i,j,k) * D(i,j,k)\n\n\n\nThe syntax shown above corresponds to exactly what you would have to write in C++ with the taco library to define tensor algebra computations. Note, however, that prior to defining a tensor algebra computation, all index variables have to be declared. This can be done as shown below:\n\n\nIndexVar i, j, k;  // Declare index variables for previous example\n\n\n\nExpressing Reductions\n\n\nIn both of the previous examples, all of the index variables are used to index into both the output and the inputs. However, it is possible for an index variable to be used to index into the inputs only, in which case the index variable is reduced (summed) over. For instance, the following example \n\n\ny(i) = A(i,j) * x(j)\n\n\n\ncan be rewritten with the summation more explicit as \ny(i) = \\sum_{j} A(i,j) \\cdot x(j)\n and demonstrates how matrix-vector multiplication can be expressed in index notation.\n\n\nNote that, in taco, reductions are assumed to be over the smallest subexpression that captures all uses of the corresponding reduction variable. For instance, the following computation \n\n\ny(i) = A(i,j) * x(j) + z(i)\n\n\n\ncan be rewritten with the summation more explicit as \n\n\n\n\ny(i) = \\big(\\sum_{j} A(i,j) \\cdot x(j)\\big) + z(i),\n\n\n\n\nwhereas the following computation \n\n\ny(i) = A(i,j) * x(j) + z(j)\n\n\n\ncan be rewritten with the summation more explicit as \n\n\n\n\ny(i) = \\sum_{j} \\big(A(i,j) \\cdot x(j) + z(i)\\big).\n\n\n\n\nPerforming the Computation\n\n\nOnce a tensor algebra computation has been defined (and all of the inputs have been \ninitialized\n), you can simply invoke the output tensor's \nevaluate\n method to perform the actual computation:\n\n\nA.evaluate();  // Perform the computation defined previously for output tensor A\n\n\n\nUnder the hood, when you invoke the \nevaluate\n method, taco first invokes the output tensor's \ncompile\n method to generate kernels that assembles the output indices (if the tensor contains any sparse dimensions) and that performs the actual computation. taco would then call the two generated kernels by invoking the output tensor's \nassemble\n and \ncompute\n methods. You can manually invoke these methods instead of calling \nevaluate\n as demonstrated below:\n\n\nA.compile();   // Generate output assembly and compute kernels \nA.assemble();  // Invoke the output assembly kernel to assemble the output indices\nA.compute();   // Invoke the compute kernel to perform the actual computation\n\n\n\nThis can be useful if you want to perform the same computation multiple times, in which case it suffices to invoke \ncompile\n once before the first time the computation is performed.\n\n\nDelayed Execuation\n\n\nIt is also possible to skip using the compiler functions entirely. Once you attempt to modify or view the output tensor, taco will automatically invoke the compiler in order to generate the data. \n\n\nIt should be noted that in order to accurately time a computation, it is necessary to invoke the compiler functions directly since relying on the delayed execution mechanism can cause a lot of prior computations to be included in the timing.", 
            "title": "Computing on Tensors"
        }, 
        {
            "location": "/computations/index.html#specifying-tensor-algebra-computations", 
            "text": "Tensor algebra computations can be expressed in taco with tensor index notation, which at a high level describes how each element in the output tensor can be computed from elements in the input tensors. As an example, matrix addition can be expressed in index notation as   A(i,j) = B(i,j) + C(i,j)  where  A ,  B , and  C  denote order-2 tensors (i.e. matrices) while  i  and  j  are index variables that represent abstract indices into the corresponding dimensions of the tensors. In words, the example above essentially states that, for every  i  and  j , the element in the  i -th row and  j -th column of the  A  should be assigned the sum of the corresponding elements in  B  and  C . Similarly, element-wise multiplication of three order-3 tensors can be expressed in index notation as follows  A(i,j,k) = B(i,j,k) * C(i,j,k) * D(i,j,k)  The syntax shown above corresponds to exactly what you would have to write in C++ with the taco library to define tensor algebra computations. Note, however, that prior to defining a tensor algebra computation, all index variables have to be declared. This can be done as shown below:  IndexVar i, j, k;  // Declare index variables for previous example", 
            "title": "Specifying Tensor Algebra Computations"
        }, 
        {
            "location": "/computations/index.html#expressing-reductions", 
            "text": "In both of the previous examples, all of the index variables are used to index into both the output and the inputs. However, it is possible for an index variable to be used to index into the inputs only, in which case the index variable is reduced (summed) over. For instance, the following example   y(i) = A(i,j) * x(j)  can be rewritten with the summation more explicit as  y(i) = \\sum_{j} A(i,j) \\cdot x(j)  and demonstrates how matrix-vector multiplication can be expressed in index notation.  Note that, in taco, reductions are assumed to be over the smallest subexpression that captures all uses of the corresponding reduction variable. For instance, the following computation   y(i) = A(i,j) * x(j) + z(i)  can be rewritten with the summation more explicit as    y(i) = \\big(\\sum_{j} A(i,j) \\cdot x(j)\\big) + z(i),   whereas the following computation   y(i) = A(i,j) * x(j) + z(j)  can be rewritten with the summation more explicit as    y(i) = \\sum_{j} \\big(A(i,j) \\cdot x(j) + z(i)\\big).", 
            "title": "Expressing Reductions"
        }, 
        {
            "location": "/computations/index.html#performing-the-computation", 
            "text": "Once a tensor algebra computation has been defined (and all of the inputs have been  initialized ), you can simply invoke the output tensor's  evaluate  method to perform the actual computation:  A.evaluate();  // Perform the computation defined previously for output tensor A  Under the hood, when you invoke the  evaluate  method, taco first invokes the output tensor's  compile  method to generate kernels that assembles the output indices (if the tensor contains any sparse dimensions) and that performs the actual computation. taco would then call the two generated kernels by invoking the output tensor's  assemble  and  compute  methods. You can manually invoke these methods instead of calling  evaluate  as demonstrated below:  A.compile();   // Generate output assembly and compute kernels \nA.assemble();  // Invoke the output assembly kernel to assemble the output indices\nA.compute();   // Invoke the compute kernel to perform the actual computation  This can be useful if you want to perform the same computation multiple times, in which case it suffices to invoke  compile  once before the first time the computation is performed.", 
            "title": "Performing the Computation"
        }, 
        {
            "location": "/computations/index.html#delayed-execuation", 
            "text": "It is also possible to skip using the compiler functions entirely. Once you attempt to modify or view the output tensor, taco will automatically invoke the compiler in order to generate the data.   It should be noted that in order to accurately time a computation, it is necessary to invoke the compiler functions directly since relying on the delayed execution mechanism can cause a lot of prior computations to be included in the timing.", 
            "title": "Delayed Execuation"
        }, 
        {
            "location": "/scheduling/index.html", 
            "text": "The scheduling language enables users to specify and compose transformations to further optimize the code generated by taco. \n\n\nConsider the following SpMV computation and associated code, which we will transform below:\n\nFormat csr({Dense,Sparse});\nTensor\ndouble\n A(\"A\", {512, 64}, csr);\nTensor\ndouble\n x(\"x\", {64}, {Dense});\nTensor\ndouble\n y(\"y\", {512}, {Dense});\n\nIndexVar i(\"i\"), j(\"j\"); \nAccess matrix = A(i, j);\ny(i) = matrix * x(j);\nIndexStmt stmt = y.getAssignment().concretize();\n\n\nfor (int32_t i = 0; i \n A1_dimension; i++) {\n    for (int32_t jA = A2_pos[i]; jA \n A2_pos[(i + 1)]; jA++) {\n        int32_t j = A2_crd[jA];\n        y_vals[i] = y_vals[i] + A_vals[jA] * x_vals[j];\n    }\n}\n\n\nPos\n\n\nThe \npos(i, ipos, access)\n transformation takes in an index variable \ni\n that iterates over the coordinate space of \naccess\n and replaces it with a derived index variable \nipos\n that iterates over the same iteration range, but with respect to the the position space. \n\n\nSince the \npos\n transformation is not valid for dense level formats, for the SpMV example, the following would result in an error:\n\nstmt = stmt.pos(i, IndexVar(\"ipos\"), matrix);\n\n\nWe could instead have: \n\nstmt = stmt.pos(j, IndexVar(\"jpos\"), matrix);\n\n\nfor (int32_t i = 0; i \n A1_dimension; i++) {\n    for (int32_t jposA = A2_pos[i]; jposA \n A2_pos[(i + 1)]; jposA++) {\n        if (jposA \n A2_pos[i] || jposA \n= A2_pos[(i + 1)])\n            continue;\n\n        int32_t j = A2_crd[jposA];\n        y_vals[i] = y_vals[i] + A_vals[jposA] * x_vals[j];\n    }\n} \n\n\nFuse\n\n\nThe \nfuse(i, j, f)\n transformation takes in two index variables \ni\n and \nj\n, where \nj\n is directly nested under \ni\n, and collapses them into a fused index variable \nf\n that iterates over the product of the coordinates \ni\n and \nj\n. \n\n\nfuse\n helps facilitate other transformations, such as iterating over the position space of several index variables, as in this SpMV example: \n\nIndexVar f(\"f\");\nstmt = stmt.fuse(i, j, f);\nstmt = stmt.pos(f, IndexVar(\"fpos\"), matrix);\n\n\nfor (int32_t fposA = 0; fposA \n A2_pos[A1_dimension]; fposA++) {\n    if (fposA \n= A2_pos[A1_dimension])\n        continue;\n\n    int32_t f = A2_crd[fposA];\n    while (fposA == A2_pos[(i_pos + 1)]) {\n        i_pos++;\n        i = i_pos;\n    }\n    y_vals[i] = y_vals[i] + A_vals[fposA] * x_vals[f];\n}\n\n\nSplit\n\n\nThe \nsplit(i, i0, i1, splitFactor)\n transformation splits (strip-mines) an index variable \ni\n into two nested index variables \ni0\n and \ni1\n. The size of the inner index variable \ni1\n is then held constant at \nsplitFactor\n, which must be a positive integer.\n\n\nFor the SpMV example, we could have: \n\nstmt = stmt.split(i, IndexVar(\"i0\"), IndexVar(\"i1\"), 16);\n\n\nfor (int32_t i0 = 0; i0 \n ((A1_dimension + 15) / 16); i0++) {\n    for (int32_t i1 = 0; i1 \n 16; i1++) {\n        int32_t i = i0 * 16 + i1;\n        if (i \n= A1_dimension)\n            continue;\n\n        for (int32_t jA = A2_pos[i]; jA \n A2_pos[(i + 1)]; jA++) {\n            int32_t j = A2_crd[jA];\n            y_vals[i] = y_vals[i] + A_vals[jA] * x_vals[j];\n        }\n    }\n}\n\n\n\n\n\n\n\n\nPrecompute\n\n\nThe \nprecompute(expr, i, iw, workspace)\n transformation, which is described in more detail \nhere\n, leverages scratchpad memories and reorders computations to  increase locality. \n\n\nGiven a subexpression \nexpr\n to precompute, an index variable \ni\n to precompute over, and an index variable \niw\n (which can be the same or different as \ni\n) to precompute with, the precomputed results are stored in the tensor variable \nworkspace\n. \n\n\nFor the SpMV example, if \nrhs\n is the right hand side of the original statement, we could have: \n\nTensorVar workspace(\"workspace\", Type(Float64, {Dimension(64)}), taco::dense);\nstmt = stmt.precompute(rhs, j, j, workspace);\n\n\nfor (int32_t i = 0; i \n A1_dimension; i++) {\n    double* restrict workspace = 0;\n    workspace = (double*)malloc(sizeof(double) * 64);\n    for (int32_t pworkspace = 0; pworkspace \n 64; pworkspace++) {\n        workspace[pworkspace] = 0.0;\n    }\n    for (int32_t jA = A2_pos[i]; jA \n A2_pos[(i + 1)]; jA++) {\n        int32_t j = A2_crd[jA];\n        workspace[j] = A_vals[jA] * x_vals[j];\n    }\n    for (int32_t j = 0; j \n 64; j++) {\n        y_vals[i] = y_vals[i] + workspace[j];\n    }\n    free(workspace);\n  }\n\n\nReorder\n\n\nThe \nreorder(vars)\n transformation takes in a new ordering for a set of index variables in the expression that are directly nested in the iteration order. \n\n\nFor the SpMV example, we could have: \n\nstmt = stmt.reorder({j, i});\n\n\nfor (int32_t jA = A2_pos[iA]; jA \n A2_pos[(iA + 1)]; jA++) {\n    int32_t j = A2_crd[jA];\n    for (int32_t i = 0; i \n A1_dimension; i++) {\n        y_vals[i] = y_vals[i] + A_vals[jA] * x_vals[j];\n    }\n }\n\n\nBound\n\n\nThe \nbound(i, ibound, bound, bound_type)\n transformation replaces an index variable \ni\n with an index variable \nibound\n that obeys a compile-time constraint on its iteration space, incorporating knowledge about the size or structured sparsity pattern of the corresponding input. The meaning of \nbound\n depends on the \nbound_type\n.\n\n\nFor the SpMV example, we could have\n\nstmt = stmt.bound(i, IndexVar(\"ibound\"), 100, BoundType::MaxExact); \n\n\nfor (int32_t ibound = 0; ibound \n 100; ibound++) {\n    for (int32_t jA = A2_pos[ibound]; jA \n A2_pos[(ibound + 1)]; jA++) {\n        int32_t j = A2_crd[jA];\n        y_vals[ibound] = y_vals[ibound] + A_vals[jA] * x_vals[j];\n    }\n}\n\n\nUnroll\n\n\nThe \nunroll(i, unrollFactor)\n transformation unrolls the loop corresponding to an index variable \ni\n by \nunrollFactor\n number of iterations, where \nunrollFactor\n is a positive integer. \n\n\nFor the SpMV example, we could have\n\nstmt = stmt.split(i, i0, i1, 32);\nstmt = stmt.unroll(i0, 4);\n\n\nif ((((A1_dimension + 31) / 32) * 32 + 32) + (((A1_dimension + 31) / 32) * 32 + 32) \n= A1_dimension) {\n    for (int32_t i0 = 0; i0 \n ((A1_dimension + 31) / 32); i0++) {\n        for (int32_t i1 = 0; i1 \n 32; i1++) {\n            int32_t i = i0 * 32 + i1;\n            if (i \n= A1_dimension)\n                continue;\n\n            for (int32_t jA = A2_pos[i]; jA \n A2_pos[(i + 1)]; jA++) {\n                int32_t j = A2_crd[jA];\n                y_vals[i] = y_vals[i] + A_vals[jA] * x_vals[j];\n            }\n        }\n    }\n}\nelse {\n    #pragma unroll 4\n    for (int32_t i0 = 0; i0 \n ((A1_dimension + 31) / 32); i0++) {\n        for (int32_t i1 = 0; i1 \n 32; i1++) {\n            int32_t i = i0 * 32 + i1;\n            for (int32_t jA = A2_pos[i]; jA \n A2_pos[(i + 1)]; jA++) {\n                int32_t j = A2_crd[jA];\n                y_vals[i] = y_vals[i] + A_vals[jA] * x_vals[j];\n            }\n        }\n    }\n}\n\n\nParallelize\n\n\nThe \nparallelize(i, parallel_unit, output_race_strategy)\n transformation tags an index variable \ni\n for parallel execution on hardware type \nparallel_unit\n. Data races are handled by an \noutput_race_strategy\n. Since the other transformations expect serial code, \nparallelize\n must come last in a series of transformations. \n\n\nFor the SpMV example, we could have\n\nstmt = stmt.parallelize(i, ParallelUnit::CPUThread, OutputRaceStrategy::NoRaces);\n\n\n#pragma omp parallel for schedule(runtime)\nfor (int32_t i = 0; i \n A1_dimension; i++) {\n    for (int32_t jA = A2_pos[i]; jA \n A2_pos[(i + 1)]; jA++) {\n        int32_t j = A2_crd[jA];\n        y_vals[i] = y_vals[i] + A_vals[jA] * x_vals[j];\n    }\n}", 
            "title": "Providing a Schedule"
        }, 
        {
            "location": "/scheduling/index.html#pos", 
            "text": "The  pos(i, ipos, access)  transformation takes in an index variable  i  that iterates over the coordinate space of  access  and replaces it with a derived index variable  ipos  that iterates over the same iteration range, but with respect to the the position space.   Since the  pos  transformation is not valid for dense level formats, for the SpMV example, the following would result in an error: stmt = stmt.pos(i, IndexVar(\"ipos\"), matrix);  We could instead have:  stmt = stmt.pos(j, IndexVar(\"jpos\"), matrix);  for (int32_t i = 0; i   A1_dimension; i++) {\n    for (int32_t jposA = A2_pos[i]; jposA   A2_pos[(i + 1)]; jposA++) {\n        if (jposA   A2_pos[i] || jposA  = A2_pos[(i + 1)])\n            continue;\n\n        int32_t j = A2_crd[jposA];\n        y_vals[i] = y_vals[i] + A_vals[jposA] * x_vals[j];\n    }\n}", 
            "title": "Pos"
        }, 
        {
            "location": "/scheduling/index.html#fuse", 
            "text": "The  fuse(i, j, f)  transformation takes in two index variables  i  and  j , where  j  is directly nested under  i , and collapses them into a fused index variable  f  that iterates over the product of the coordinates  i  and  j .   fuse  helps facilitate other transformations, such as iterating over the position space of several index variables, as in this SpMV example:  IndexVar f(\"f\");\nstmt = stmt.fuse(i, j, f);\nstmt = stmt.pos(f, IndexVar(\"fpos\"), matrix);  for (int32_t fposA = 0; fposA   A2_pos[A1_dimension]; fposA++) {\n    if (fposA  = A2_pos[A1_dimension])\n        continue;\n\n    int32_t f = A2_crd[fposA];\n    while (fposA == A2_pos[(i_pos + 1)]) {\n        i_pos++;\n        i = i_pos;\n    }\n    y_vals[i] = y_vals[i] + A_vals[fposA] * x_vals[f];\n}", 
            "title": "Fuse"
        }, 
        {
            "location": "/scheduling/index.html#split", 
            "text": "The  split(i, i0, i1, splitFactor)  transformation splits (strip-mines) an index variable  i  into two nested index variables  i0  and  i1 . The size of the inner index variable  i1  is then held constant at  splitFactor , which must be a positive integer.  For the SpMV example, we could have:  stmt = stmt.split(i, IndexVar(\"i0\"), IndexVar(\"i1\"), 16);  for (int32_t i0 = 0; i0   ((A1_dimension + 15) / 16); i0++) {\n    for (int32_t i1 = 0; i1   16; i1++) {\n        int32_t i = i0 * 16 + i1;\n        if (i  = A1_dimension)\n            continue;\n\n        for (int32_t jA = A2_pos[i]; jA   A2_pos[(i + 1)]; jA++) {\n            int32_t j = A2_crd[jA];\n            y_vals[i] = y_vals[i] + A_vals[jA] * x_vals[j];\n        }\n    }\n}", 
            "title": "Split"
        }, 
        {
            "location": "/scheduling/index.html#precompute", 
            "text": "The  precompute(expr, i, iw, workspace)  transformation, which is described in more detail  here , leverages scratchpad memories and reorders computations to  increase locality.   Given a subexpression  expr  to precompute, an index variable  i  to precompute over, and an index variable  iw  (which can be the same or different as  i ) to precompute with, the precomputed results are stored in the tensor variable  workspace .   For the SpMV example, if  rhs  is the right hand side of the original statement, we could have:  TensorVar workspace(\"workspace\", Type(Float64, {Dimension(64)}), taco::dense);\nstmt = stmt.precompute(rhs, j, j, workspace);  for (int32_t i = 0; i   A1_dimension; i++) {\n    double* restrict workspace = 0;\n    workspace = (double*)malloc(sizeof(double) * 64);\n    for (int32_t pworkspace = 0; pworkspace   64; pworkspace++) {\n        workspace[pworkspace] = 0.0;\n    }\n    for (int32_t jA = A2_pos[i]; jA   A2_pos[(i + 1)]; jA++) {\n        int32_t j = A2_crd[jA];\n        workspace[j] = A_vals[jA] * x_vals[j];\n    }\n    for (int32_t j = 0; j   64; j++) {\n        y_vals[i] = y_vals[i] + workspace[j];\n    }\n    free(workspace);\n  }", 
            "title": "Precompute"
        }, 
        {
            "location": "/scheduling/index.html#reorder", 
            "text": "The  reorder(vars)  transformation takes in a new ordering for a set of index variables in the expression that are directly nested in the iteration order.   For the SpMV example, we could have:  stmt = stmt.reorder({j, i});  for (int32_t jA = A2_pos[iA]; jA   A2_pos[(iA + 1)]; jA++) {\n    int32_t j = A2_crd[jA];\n    for (int32_t i = 0; i   A1_dimension; i++) {\n        y_vals[i] = y_vals[i] + A_vals[jA] * x_vals[j];\n    }\n }", 
            "title": "Reorder"
        }, 
        {
            "location": "/scheduling/index.html#bound", 
            "text": "The  bound(i, ibound, bound, bound_type)  transformation replaces an index variable  i  with an index variable  ibound  that obeys a compile-time constraint on its iteration space, incorporating knowledge about the size or structured sparsity pattern of the corresponding input. The meaning of  bound  depends on the  bound_type .  For the SpMV example, we could have stmt = stmt.bound(i, IndexVar(\"ibound\"), 100, BoundType::MaxExact);   for (int32_t ibound = 0; ibound   100; ibound++) {\n    for (int32_t jA = A2_pos[ibound]; jA   A2_pos[(ibound + 1)]; jA++) {\n        int32_t j = A2_crd[jA];\n        y_vals[ibound] = y_vals[ibound] + A_vals[jA] * x_vals[j];\n    }\n}", 
            "title": "Bound"
        }, 
        {
            "location": "/scheduling/index.html#unroll", 
            "text": "The  unroll(i, unrollFactor)  transformation unrolls the loop corresponding to an index variable  i  by  unrollFactor  number of iterations, where  unrollFactor  is a positive integer.   For the SpMV example, we could have stmt = stmt.split(i, i0, i1, 32);\nstmt = stmt.unroll(i0, 4);  if ((((A1_dimension + 31) / 32) * 32 + 32) + (((A1_dimension + 31) / 32) * 32 + 32)  = A1_dimension) {\n    for (int32_t i0 = 0; i0   ((A1_dimension + 31) / 32); i0++) {\n        for (int32_t i1 = 0; i1   32; i1++) {\n            int32_t i = i0 * 32 + i1;\n            if (i  = A1_dimension)\n                continue;\n\n            for (int32_t jA = A2_pos[i]; jA   A2_pos[(i + 1)]; jA++) {\n                int32_t j = A2_crd[jA];\n                y_vals[i] = y_vals[i] + A_vals[jA] * x_vals[j];\n            }\n        }\n    }\n}\nelse {\n    #pragma unroll 4\n    for (int32_t i0 = 0; i0   ((A1_dimension + 31) / 32); i0++) {\n        for (int32_t i1 = 0; i1   32; i1++) {\n            int32_t i = i0 * 32 + i1;\n            for (int32_t jA = A2_pos[i]; jA   A2_pos[(i + 1)]; jA++) {\n                int32_t j = A2_crd[jA];\n                y_vals[i] = y_vals[i] + A_vals[jA] * x_vals[j];\n            }\n        }\n    }\n}", 
            "title": "Unroll"
        }, 
        {
            "location": "/scheduling/index.html#parallelize", 
            "text": "The  parallelize(i, parallel_unit, output_race_strategy)  transformation tags an index variable  i  for parallel execution on hardware type  parallel_unit . Data races are handled by an  output_race_strategy . Since the other transformations expect serial code,  parallelize  must come last in a series of transformations.   For the SpMV example, we could have stmt = stmt.parallelize(i, ParallelUnit::CPUThread, OutputRaceStrategy::NoRaces);  #pragma omp parallel for schedule(runtime)\nfor (int32_t i = 0; i   A1_dimension; i++) {\n    for (int32_t jA = A2_pos[i]; jA   A2_pos[(i + 1)]; jA++) {\n        int32_t j = A2_crd[jA];\n        y_vals[i] = y_vals[i] + A_vals[jA] * x_vals[j];\n    }\n}", 
            "title": "Parallelize"
        }, 
        {
            "location": "/tutorial/index.html", 
            "text": "The linked Jupyter notebooks proivde an interactive introduction to the Python TACO library, including how to initialize tensors, define mode formats, and perform computations. There are three notebooks, which differ mainly in the final extended example: \nSpMV\n (useful for scientific computing), \nSDDMM\n (machine learning), and \nMTTKRP\n (data analytics).\n\n\nThese notebooks are hosted online and \ndo not require any installation\n; i.e., they can be run without having PyTaco, Jupyter, or even Python locally. However, they may take a minute or two to build.\n\n\nIf, on the other hand, you would like to run the notebooks on your computer, please do the following: \n\n\n\n\nInstall \nPython 3\n. \n\n\nIntall \nJupyter\n.\n\n\nInstall \nNumPy and SciPy\n.\n\n\nInstall \nPyTaco\n. Follow the instructions to build taco with the Python API.\n\n\nClone the repository with the \ntutorials\n.", 
            "title": "Tutorial"
        }, 
        {
            "location": "/pytensors/index.html", 
            "text": "Declaring Tensors\n\n\npytaco.tensor\n objects, which represent mathematical tensors, form the core of\nthe TACO Python library. You can can declare a new tensor by specifying the\nsizes of each dimension, the \nformat\n\nthat will be used to store the tensor, and the\n\ndatatype\n of the tensor's nonzero elements:\n\n\n# Import the TACO Python library\nimport pytaco as pt\nfrom pytaco import dense, compressed\n\n# Declare a new tensor of double-precision floats with dimensions \n# 512 x 64 x 2048, stored as a dense-sparse-sparse tensor\nA = pt.tensor([512, 64, 2048], pt.format([dense, compressed, compressed]), pt.float64)\n\n\n\nThe datatype can be omitted, in which case TACO will default to using\n\npt.float32\n to store the tensor's nonzero elements:\n\n\n# Declare the same tensor as before\nA = pt.tensor([512, 64, 2048], pt.format([dense, compressed, compressed]))\n\n\n\nInstead of specifying a format that is tied to the number of dimensions that a\ntensor has, we can simply specify whether all dimensions are dense or sparse:\n\n\n# Declare a tensor where all dimensions are dense\nA = pt.tensor([512, 64, 2048], dense)\n\n# Declare a tensor where all dimensions are sparse\nB = pt.tensor([512, 64, 2048], compressed)\n\n\n\nScalars, which correspond to tensors that have zero dimension, can be declared\nand initialized with an arbitrary value as demonstrated below:\n\n\n# Declare a scalar\naplha = pt.tensor(42.0)\n\n\n\nDefining Tensor Formats\n\n\nConceptually, you can think of a tensor as a tree where each level (excluding\nthe root) corresponding to a dimension of the tensor.  Each path from the root\nto a leaf node represents the coordinates of a tensor element and its\ncorresponding value.  Which dimension of the tensor each level of the tree\ncorresponds to is determined by the order in which tensor dimensions are\nstored.\n\n\nTACO uses a novel scheme that can describe different storage formats for a\ntensor by specifying the order in which tensor dimensions are stored and\nwhether each dimension is sparse or dense.  A sparse (compressed) dimension\nstores only the subset of the dimension that contains non-zero values, using\nindex arrays that are found in the compressed sparse row (CSR) matrix format.\nA dense dimension, on the other hand, conceptually stores both zeros and\nnon-zeros.  This scheme is flexibile enough to express many commonly-used\ntensor storage formats:\n\n\nimport pytaco as pt\nfrom pytaco import dense, compressed\n\ndm   = pt.format([dense, dense])                        # (Row-major) dense matrix format\ncsr  = pt.format([dense, compressed])                   # Compressed sparse row matrix format\ncsc  = pt.format([dense, compressed], [1, 0])           # Compressed sparse column matrix format\ndcsr = pt.format([compressed, compressed], [1, 0])      # Doubly compressed sparse column matrix format\ncsf  = pt.format([compressed, compressed, compressed])  # Compressed sparse fiber tensor format\n\n\n\nAs demonstrated above, you can define a new tensor storage format by creating a\n\npytaco.format\n object.  This requires specifying whether each tensor dimension\nis dense or sparse as well as (optionally) the order in which dimensions should\nbe stored.  TACO also predefines some common tensor formats (including \n\npt.csr\n and \npt.csc\n) that you can use out of the box.\n\n\nInitializing Tensors\n\n\nTensors can be made by using python indexing syntax. For example, one may write\nthe following: You can initialize a tensor by calling its \ninsert\n method to\nadd a nonzero element to the tensor. The \ninsert\n method takes two arguments:\na list specifying the coordinates of the nonzero element to be added and the\nvalue to be inserted at that coordinate:\n\n\n# Declare a sparse tensor\nA = pt.tensor([512, 64, 2048], compressed)\n\n# Set A(0, 1, 0) = 42.0\nA.insert([0, 1, 0], 42.0)\n\n\n\nIf multiple elements are inserted at the same coordinates, they are summed \ntogether:\n\n\n# Declare a sparse tensor\nA = pt.tensor([512, 64, 2048], compressed)\n\n# Set A(0, 1, 0) = 42.0 + 24.0 = 66.0\nA.insert([0, 1, 0], 42.0)\nA.insert([0, 1, 0], 24.0)\n\n\n\nThe \ninsert\n method adds the inserted nonzero element to a temporary buffer.\nBefore a tensor can actually be used in a computation though, the \npack\n method\nmust be invoked to pack the tensor into the storage format that was specified\nwhen the tensor was first declared.  TACO will automatically do this\nimmediately before the tensor is used in a computation.  You can also manually\ninvoke \npack\n though if you need full control over when exactly that is done:\n\n\nA.pack()\n\n\n\nYou can then iterate over the nonzero elements of the tensor as follows:\n\n\nfor coordinates, val in A:\n  print(val)\n\n\n\nFile I/O\n\n\nRather than manually constructing a tensor, you can load tensors directly from\nfile by invoking the \npytaco.read\n function:\n\n\n# Load a dense-sparse-sparse tensor from file \"A.tns\"\nA = pt.read(\"A.tns\", pt.format([dense, compressed, compressed]))\n\n\n\nBy default, \npytaco.read\n returns a tensor that has already been packed into\nthe specified storage format. You can optionally pass a Boolean flag as an\nargument to indicate whether the returned tensor should be packed or not: \n\n\n# Load an unpacked tensor from file \"A.tns\"\nA = pt.read(\"A.tns\", format([dense, compressed, compressed]), false)\n\n\n\nThe loaded tensor will then remain unpacked until the \npack\n method is manually \ninvoked or a computation that uses the tensor is performed.\n\n\nYou can also write a tensor directly to file by invoking the \npytaco.write\n\nfunction:\n\n\n# Write tensor A to file \"A.tns\"\npt.write(\"A.tns\", A)\n\n\n\nTACO supports loading tensors from and storing tensors to the following file\nformats:\n\n\n\n\nMatrix Market (Coordinate) Format (.mtx)\n\n\nRutherford-Boeing Format (.rb)\n\n\nFROSTT Format (.tns)\n\n\n\n\nNumPy and SciPy I/O\n\n\nTensors can also be initialized with either NumPy arrays or SciPy sparse (CSR \nor CSC) matrices:\n\n\nimport pytaco as pt\nimport numpy as np\nimport scipy.sparse\n\n# Assuming SciPy matrix is stored in CSR\nsparse_matrix = scipy.sparse.load_npz('sparse_matrix.npz')\n\n# Cast the matrix as a TACO tensor (also stored in CSR)\ntaco_tensor = pt.from_sp_csr(sparse_matrix)\n\n# We can also load a NumPy array\nnp_array = np.load('arr.npy')\n\n# And initialize a TACO tensor from this array\ndense_tensor = pt.from_array(np_array)\n\n\n\nWe can also export TACO tensors to either NumPy arrays or SciPy sparse\nmatrices:\n\n\n# Convert the tensor to a SciPy CSR matrix\nsparse_matrix = taco_tensor.to_sp_csr()\n\n# Convert the tensor to a NumPy array\nnp_array = dense_tensor.to_array()", 
            "title": "Defining Tensors"
        }, 
        {
            "location": "/pytensors/index.html#declaring-tensors", 
            "text": "pytaco.tensor  objects, which represent mathematical tensors, form the core of\nthe TACO Python library. You can can declare a new tensor by specifying the\nsizes of each dimension, the  format \nthat will be used to store the tensor, and the datatype  of the tensor's nonzero elements:  # Import the TACO Python library\nimport pytaco as pt\nfrom pytaco import dense, compressed\n\n# Declare a new tensor of double-precision floats with dimensions \n# 512 x 64 x 2048, stored as a dense-sparse-sparse tensor\nA = pt.tensor([512, 64, 2048], pt.format([dense, compressed, compressed]), pt.float64)  The datatype can be omitted, in which case TACO will default to using pt.float32  to store the tensor's nonzero elements:  # Declare the same tensor as before\nA = pt.tensor([512, 64, 2048], pt.format([dense, compressed, compressed]))  Instead of specifying a format that is tied to the number of dimensions that a\ntensor has, we can simply specify whether all dimensions are dense or sparse:  # Declare a tensor where all dimensions are dense\nA = pt.tensor([512, 64, 2048], dense)\n\n# Declare a tensor where all dimensions are sparse\nB = pt.tensor([512, 64, 2048], compressed)  Scalars, which correspond to tensors that have zero dimension, can be declared\nand initialized with an arbitrary value as demonstrated below:  # Declare a scalar\naplha = pt.tensor(42.0)", 
            "title": "Declaring Tensors"
        }, 
        {
            "location": "/pytensors/index.html#defining-tensor-formats", 
            "text": "Conceptually, you can think of a tensor as a tree where each level (excluding\nthe root) corresponding to a dimension of the tensor.  Each path from the root\nto a leaf node represents the coordinates of a tensor element and its\ncorresponding value.  Which dimension of the tensor each level of the tree\ncorresponds to is determined by the order in which tensor dimensions are\nstored.  TACO uses a novel scheme that can describe different storage formats for a\ntensor by specifying the order in which tensor dimensions are stored and\nwhether each dimension is sparse or dense.  A sparse (compressed) dimension\nstores only the subset of the dimension that contains non-zero values, using\nindex arrays that are found in the compressed sparse row (CSR) matrix format.\nA dense dimension, on the other hand, conceptually stores both zeros and\nnon-zeros.  This scheme is flexibile enough to express many commonly-used\ntensor storage formats:  import pytaco as pt\nfrom pytaco import dense, compressed\n\ndm   = pt.format([dense, dense])                        # (Row-major) dense matrix format\ncsr  = pt.format([dense, compressed])                   # Compressed sparse row matrix format\ncsc  = pt.format([dense, compressed], [1, 0])           # Compressed sparse column matrix format\ndcsr = pt.format([compressed, compressed], [1, 0])      # Doubly compressed sparse column matrix format\ncsf  = pt.format([compressed, compressed, compressed])  # Compressed sparse fiber tensor format  As demonstrated above, you can define a new tensor storage format by creating a pytaco.format  object.  This requires specifying whether each tensor dimension\nis dense or sparse as well as (optionally) the order in which dimensions should\nbe stored.  TACO also predefines some common tensor formats (including  pt.csr  and  pt.csc ) that you can use out of the box.", 
            "title": "Defining Tensor Formats"
        }, 
        {
            "location": "/pytensors/index.html#initializing-tensors", 
            "text": "Tensors can be made by using python indexing syntax. For example, one may write\nthe following: You can initialize a tensor by calling its  insert  method to\nadd a nonzero element to the tensor. The  insert  method takes two arguments:\na list specifying the coordinates of the nonzero element to be added and the\nvalue to be inserted at that coordinate:  # Declare a sparse tensor\nA = pt.tensor([512, 64, 2048], compressed)\n\n# Set A(0, 1, 0) = 42.0\nA.insert([0, 1, 0], 42.0)  If multiple elements are inserted at the same coordinates, they are summed \ntogether:  # Declare a sparse tensor\nA = pt.tensor([512, 64, 2048], compressed)\n\n# Set A(0, 1, 0) = 42.0 + 24.0 = 66.0\nA.insert([0, 1, 0], 42.0)\nA.insert([0, 1, 0], 24.0)  The  insert  method adds the inserted nonzero element to a temporary buffer.\nBefore a tensor can actually be used in a computation though, the  pack  method\nmust be invoked to pack the tensor into the storage format that was specified\nwhen the tensor was first declared.  TACO will automatically do this\nimmediately before the tensor is used in a computation.  You can also manually\ninvoke  pack  though if you need full control over when exactly that is done:  A.pack()  You can then iterate over the nonzero elements of the tensor as follows:  for coordinates, val in A:\n  print(val)", 
            "title": "Initializing Tensors"
        }, 
        {
            "location": "/pytensors/index.html#file-io", 
            "text": "Rather than manually constructing a tensor, you can load tensors directly from\nfile by invoking the  pytaco.read  function:  # Load a dense-sparse-sparse tensor from file \"A.tns\"\nA = pt.read(\"A.tns\", pt.format([dense, compressed, compressed]))  By default,  pytaco.read  returns a tensor that has already been packed into\nthe specified storage format. You can optionally pass a Boolean flag as an\nargument to indicate whether the returned tensor should be packed or not:   # Load an unpacked tensor from file \"A.tns\"\nA = pt.read(\"A.tns\", format([dense, compressed, compressed]), false)  The loaded tensor will then remain unpacked until the  pack  method is manually \ninvoked or a computation that uses the tensor is performed.  You can also write a tensor directly to file by invoking the  pytaco.write \nfunction:  # Write tensor A to file \"A.tns\"\npt.write(\"A.tns\", A)  TACO supports loading tensors from and storing tensors to the following file\nformats:   Matrix Market (Coordinate) Format (.mtx)  Rutherford-Boeing Format (.rb)  FROSTT Format (.tns)", 
            "title": "File I/O"
        }, 
        {
            "location": "/pytensors/index.html#numpy-and-scipy-io", 
            "text": "Tensors can also be initialized with either NumPy arrays or SciPy sparse (CSR \nor CSC) matrices:  import pytaco as pt\nimport numpy as np\nimport scipy.sparse\n\n# Assuming SciPy matrix is stored in CSR\nsparse_matrix = scipy.sparse.load_npz('sparse_matrix.npz')\n\n# Cast the matrix as a TACO tensor (also stored in CSR)\ntaco_tensor = pt.from_sp_csr(sparse_matrix)\n\n# We can also load a NumPy array\nnp_array = np.load('arr.npy')\n\n# And initialize a TACO tensor from this array\ndense_tensor = pt.from_array(np_array)  We can also export TACO tensors to either NumPy arrays or SciPy sparse\nmatrices:  # Convert the tensor to a SciPy CSR matrix\nsparse_matrix = taco_tensor.to_sp_csr()\n\n# Convert the tensor to a NumPy array\nnp_array = dense_tensor.to_array()", 
            "title": "NumPy and SciPy I/O"
        }, 
        {
            "location": "/pycomputations/index.html", 
            "text": "Specifying Tensor Algebra Computations\n\n\nTensor algebra computations can be expressed in TACO using tensor index\nnotation, which at a high level describes how each element in the result tensor\ncan be computed from elements in the operand tensors. As an example, matrix\naddition can be expressed in index notation as \n\n\n\n\nA_{ij} = B_{ij} + C_{ij}\n\n\n\n\nwhere \nA\n, \nB\n, and \nC\n denote two-dimensional tensors (i.e., matrices)\nwhile \ni\n and \nj\n are index variables that represent abstract indices into\nthe corresponding dimensions of the tensors.  In plain English, the example\nabove essentially states that, for every \ni\n and \nj\n, the element in the\n\ni\n-th row and \nj\n-th column of \nA\n should be assigned the sum of the\ncorresponding elements in \nB\n and \nC\n. Similarly, element-wise\nmultiplication of three tensors can be expressed in index notation as \n\n\n\n\nA_{ijk} = B_{ijk} \\cdot C_{ijk} \\cdot D_{ijk}.\n\n\n\n\nTo define the same computation using the TACO Python library, we can write very\nsimilar code, with the main difference being that we also have to explicitly\ndeclare the index variables beforehand:\n\n\ni, j, k = pytaco.index_var(), pytaco.index_var(), pytaco.index_var()\nA[i,j,k] = B[i,j,k] * C[i,j,k] * D[i,j,k]\n\n\n\nThis can also be rewritten more compactly as\n\n\ni, j, k = pytaco.get_index_vars(3)\nA[i,j,k] = B[i,j,k] * C[i,j,k] * D[i,j,k]\n\n\n\n\n\nNote\n\n\nAccesses to scalars also require the square brackets notation.  Since\nscalars are equivalent to tensors with zero dimension, \nNone\n must be\nexplicitly specified as indices to indicate that no index variable is\nneeded to access a scalar.  As an example, the following expresses the\naddition of two scalars \nbeta\n and \ngamma\n:\n\n\nalpha[None] = beta[None] + gamma[None]\n\n\n\n\n\n\n\nWarning\n\n\nTACO currently does not support computations that have a tensor as both an \noperand and the result, such as the following:\n\n\na[i] = a[i] * b[i]\n\n\n\nSuch computations can be rewritten using explicit temporaries as follows:\n\n\nt[i] = a[i] * b[i]\na[i] = t[i]\n\n\n\n\n\n\n\nWarning\n\n\nTACO currently does not support using the same index variable to index into \nmultiple dimensions of the same tensor operand (e.g., \nA[i,i]\n).\n\n\n\n\nExpressing Reductions\n\n\nIn all of the previous examples, all the index variables are used to index into\nboth the result and the operands of a computation.  It is also possible for\nan index variable to be used to index into the operands only, in which case the\ndimension indexed by that index variable is reduced (summed) over. For \ninstance, the computation \n\n\n\n\ny_{i} = A_{ij} \\cdot x_{j}\n\n\n\n\ncan be rewritten with the summation more explicit as \n\n\n\n\ny_{i} = \\sum_{j} A_{ij} \\cdot x_j\n\n\n\n\nand demonstrates how matrix-vector multiplication can be expressed in index\nnotation.  Both forms are supported by TACO:\n\n\ni, j = pytaco.get_index_vars(2)\n\ny[i] = A[i,j] * x[j]\ny[i] = pytaco.sum(j, A[i,j] * x[j])\n\n\n\nReductions that are not explicitly expressed are assumed to be over the\nsmallest subexpression that captures all uses of the corresponding reduction\nvariable. For instance, the computation \n\n\n\n\ny_{i} = A_{ij} \\cdot x_{j} + z_{i}\n\n\n\n\nis equivalent to \n\n\n\n\ny_i = \\big(\\sum_{j} A_{ij} \\cdot x_j\\big) + z_i,\n\n\n\n\nwhereas the computation \n\n\n\n\ny_{i} = A_{ij} \\cdot x_{j} + z_{j}\n\n\n\n\nis equivalent to \n\n\n\n\ny_i = \\sum_{j} \\big(A_{ij} \\cdot x_j + z_j\\big).\n\n\n\n\nExpressing Broadcasts\n\n\nTACO supports computations that broadcasts tensors along any number of\ndimensions.  The following example, for instance, broadcasts the vector \nc\n \nalong the row dimension of matrix \nB\n, adding \nc\n to each row of \nB\n:\n\n\nA[i, j] =  B[i, j] + c[j]\n\n\n\nHowever, TACO does not support NumPy-style broadcasting of dimensions that have \na size of one.  For example, the following is not allowed:\n\n\nA = pt.tensor([3,3])\nB = pt.tensor([3,3])\nC = pt.tensor([3,1])\ni, j = pt.get_index_vars(2)\n\nA[i, j] =  B[i, j] + C[i, j]  # ERROR!!\n\n\n\nExpressing Transposes\n\n\nComputations that transpose tensors can be expressed by rearranging the order \nin which index variables are used to access tensor operands.  The following\nexample, for instance, adds matrix \nB\n to the transpose of matrix \nC\n and\nstores the result in matrix \nA\n:\n\n\nA = pt.tensor([3,3], pt.format([dense, dense]))\nB = pt.tensor([3,3], pt.format([dense, dense]))\nC = pt.tensor([3,3], pt.format([dense, dense]))\ni, j = pt.get_index_vars(2)\n\nA[i,j] = B[i,j] + C[j,i]\n\n\n\nNote, however, that sparse dimensions of tensor operands impose dependencies on\nthe order in which they can be accessed, based on the order in which they are\nstored in the operand formats.  This means, for instance, that if \nB\n is a CSR\nmatrix, then \nB[i,j]\n requires that the dimension indexed by \ni\n be accessed\nbefore the dimension indexed by \nj\n.  TACO does not support any computation\nwhere these constraints form a cyclic dependency.  So the same computation from\nbefore is not supported for CSR matrices, since the access of \nB\n requires that\n\ni\n be accessed before \nj\n but the access of \nC\n requires that \nj\n be accessed\nbefore \ni\n:\n\n\nA = pt.tensor([3,3], pt.format([dense, compressed]))\nB = pt.tensor([3,3], pt.format([dense, compressed]))\nC = pt.tensor([3,3], pt.format([dense, compressed]))\ni, j = pt.get_index_vars(2)\n\nA[i,j] = B[i,j] + C[j,i]  # ERROR!!\n\n\n\nAs an alternative, you can first explicitly transpose \nC\n by invoking its\n\ntranspose\n method, storing the result in a temporary, and then perform the\naddition with the already-transposed temporary:\n\n\nA = pt.tensor([3,3], pt.format([dense, compressed]))\nB = pt.tensor([3,3], pt.format([dense, compressed]))\nC = pt.tensor([3,3], pt.format([dense, compressed]))\ni, j = pt.get_index_vars(2)\n\nCt = C.transpose([1, 0])  # Ct is also stored in the CSR format\nA[i,j] = B[i,j] + Ct[i,j]\n\n\n\nSimilarly, the following computation is not supported for the same reason that\nthe access of \nB\n, which is stored in row-major order, requires \ni\n to be\naccessed before \nj\n but the access of \nC\n, which is stored in column-major\norder, requires \nj\n to be accessed before \ni\n:\n\n\nA = pt.tensor([3,3], pt.format([dense, compressed]))\nB = pt.tensor([3,3], pt.format([dense, compressed]))\nC = pt.tensor([3,3], pt.format([dense, compressed], [1, 0]))\ni, j = pt.get_index_vars(2)\n\nA[i,j] = B[i,j] + C[i,j]  # ERROR!!\n\n\n\nWe can again perform the same computation by invoking \ntranspose\n, this time to\nrepack \nC\n into the same CSR format as \nA\n and \nB\n before computing the \naddition:\n\n\nA = pt.tensor([3,3], pt.format([dense, compressed]))\nB = pt.tensor([3,3], pt.format([dense, compressed]))\nC = pt.tensor([3,3], pt.format([dense, compressed], [1, 0]))\ni, j = pt.get_index_vars(2)\n\nCp = C.transpose([0, 1], pt.format([dense, compressed]))  # Store a copy of C in the CSR format\nA[i,j] = B[i,j] + Cp[i,j]\n\n\n\nPerforming the Computation\n\n\nOnce a tensor algebra computation has been defined, you can simply invoke the\nresult tensor's \nevaluate\n method to perform the actual computation:\n\n\nA.evaluate()\n\n\n\nUnder the hood, TACO will first invoke the result tensor's \ncompile\n\nmethod to generate code that performs the computation.  TACO will then perform \nthe actual computation by first invoking \nassemble\n to compute the sparsity \nstructure of the result and subsequently invoking \ncompute\n to compute the \nvalues of the result's nonzero elements.  Of course, you can also manually \ninvoke these methods in order to more precisely control when each step happens:\n\n\nA.compile()\nA.assemble()\nA.compute()\n\n\n\nIf you define a computation and then access the result without first manually\ninvoking \nevaluate\n or \ncompile\n/\nassemble\n/\ncompute\n, TACO will automatically\ninvoke the computation immediately before the result is accessed.  In the\nfollowing example, for instance, TACO will automatically generate code to\ncompute the vector addition and then also actually perform the computation\nright before \na[0]\n is printed:\n\n\na[i] = b[i] + c[i]\nprint(a[0])", 
            "title": "Computing on Tensors"
        }, 
        {
            "location": "/pycomputations/index.html#specifying-tensor-algebra-computations", 
            "text": "Tensor algebra computations can be expressed in TACO using tensor index\nnotation, which at a high level describes how each element in the result tensor\ncan be computed from elements in the operand tensors. As an example, matrix\naddition can be expressed in index notation as    A_{ij} = B_{ij} + C_{ij}   where  A ,  B , and  C  denote two-dimensional tensors (i.e., matrices)\nwhile  i  and  j  are index variables that represent abstract indices into\nthe corresponding dimensions of the tensors.  In plain English, the example\nabove essentially states that, for every  i  and  j , the element in the i -th row and  j -th column of  A  should be assigned the sum of the\ncorresponding elements in  B  and  C . Similarly, element-wise\nmultiplication of three tensors can be expressed in index notation as    A_{ijk} = B_{ijk} \\cdot C_{ijk} \\cdot D_{ijk}.   To define the same computation using the TACO Python library, we can write very\nsimilar code, with the main difference being that we also have to explicitly\ndeclare the index variables beforehand:  i, j, k = pytaco.index_var(), pytaco.index_var(), pytaco.index_var()\nA[i,j,k] = B[i,j,k] * C[i,j,k] * D[i,j,k]  This can also be rewritten more compactly as  i, j, k = pytaco.get_index_vars(3)\nA[i,j,k] = B[i,j,k] * C[i,j,k] * D[i,j,k]   Note  Accesses to scalars also require the square brackets notation.  Since\nscalars are equivalent to tensors with zero dimension,  None  must be\nexplicitly specified as indices to indicate that no index variable is\nneeded to access a scalar.  As an example, the following expresses the\naddition of two scalars  beta  and  gamma :  alpha[None] = beta[None] + gamma[None]    Warning  TACO currently does not support computations that have a tensor as both an \noperand and the result, such as the following:  a[i] = a[i] * b[i]  Such computations can be rewritten using explicit temporaries as follows:  t[i] = a[i] * b[i]\na[i] = t[i]    Warning  TACO currently does not support using the same index variable to index into \nmultiple dimensions of the same tensor operand (e.g.,  A[i,i] ).", 
            "title": "Specifying Tensor Algebra Computations"
        }, 
        {
            "location": "/pycomputations/index.html#expressing-reductions", 
            "text": "In all of the previous examples, all the index variables are used to index into\nboth the result and the operands of a computation.  It is also possible for\nan index variable to be used to index into the operands only, in which case the\ndimension indexed by that index variable is reduced (summed) over. For \ninstance, the computation    y_{i} = A_{ij} \\cdot x_{j}   can be rewritten with the summation more explicit as    y_{i} = \\sum_{j} A_{ij} \\cdot x_j   and demonstrates how matrix-vector multiplication can be expressed in index\nnotation.  Both forms are supported by TACO:  i, j = pytaco.get_index_vars(2)\n\ny[i] = A[i,j] * x[j]\ny[i] = pytaco.sum(j, A[i,j] * x[j])  Reductions that are not explicitly expressed are assumed to be over the\nsmallest subexpression that captures all uses of the corresponding reduction\nvariable. For instance, the computation    y_{i} = A_{ij} \\cdot x_{j} + z_{i}   is equivalent to    y_i = \\big(\\sum_{j} A_{ij} \\cdot x_j\\big) + z_i,   whereas the computation    y_{i} = A_{ij} \\cdot x_{j} + z_{j}   is equivalent to    y_i = \\sum_{j} \\big(A_{ij} \\cdot x_j + z_j\\big).", 
            "title": "Expressing Reductions"
        }, 
        {
            "location": "/pycomputations/index.html#expressing-broadcasts", 
            "text": "TACO supports computations that broadcasts tensors along any number of\ndimensions.  The following example, for instance, broadcasts the vector  c  \nalong the row dimension of matrix  B , adding  c  to each row of  B :  A[i, j] =  B[i, j] + c[j]  However, TACO does not support NumPy-style broadcasting of dimensions that have \na size of one.  For example, the following is not allowed:  A = pt.tensor([3,3])\nB = pt.tensor([3,3])\nC = pt.tensor([3,1])\ni, j = pt.get_index_vars(2)\n\nA[i, j] =  B[i, j] + C[i, j]  # ERROR!!", 
            "title": "Expressing Broadcasts"
        }, 
        {
            "location": "/pycomputations/index.html#expressing-transposes", 
            "text": "Computations that transpose tensors can be expressed by rearranging the order \nin which index variables are used to access tensor operands.  The following\nexample, for instance, adds matrix  B  to the transpose of matrix  C  and\nstores the result in matrix  A :  A = pt.tensor([3,3], pt.format([dense, dense]))\nB = pt.tensor([3,3], pt.format([dense, dense]))\nC = pt.tensor([3,3], pt.format([dense, dense]))\ni, j = pt.get_index_vars(2)\n\nA[i,j] = B[i,j] + C[j,i]  Note, however, that sparse dimensions of tensor operands impose dependencies on\nthe order in which they can be accessed, based on the order in which they are\nstored in the operand formats.  This means, for instance, that if  B  is a CSR\nmatrix, then  B[i,j]  requires that the dimension indexed by  i  be accessed\nbefore the dimension indexed by  j .  TACO does not support any computation\nwhere these constraints form a cyclic dependency.  So the same computation from\nbefore is not supported for CSR matrices, since the access of  B  requires that i  be accessed before  j  but the access of  C  requires that  j  be accessed\nbefore  i :  A = pt.tensor([3,3], pt.format([dense, compressed]))\nB = pt.tensor([3,3], pt.format([dense, compressed]))\nC = pt.tensor([3,3], pt.format([dense, compressed]))\ni, j = pt.get_index_vars(2)\n\nA[i,j] = B[i,j] + C[j,i]  # ERROR!!  As an alternative, you can first explicitly transpose  C  by invoking its transpose  method, storing the result in a temporary, and then perform the\naddition with the already-transposed temporary:  A = pt.tensor([3,3], pt.format([dense, compressed]))\nB = pt.tensor([3,3], pt.format([dense, compressed]))\nC = pt.tensor([3,3], pt.format([dense, compressed]))\ni, j = pt.get_index_vars(2)\n\nCt = C.transpose([1, 0])  # Ct is also stored in the CSR format\nA[i,j] = B[i,j] + Ct[i,j]  Similarly, the following computation is not supported for the same reason that\nthe access of  B , which is stored in row-major order, requires  i  to be\naccessed before  j  but the access of  C , which is stored in column-major\norder, requires  j  to be accessed before  i :  A = pt.tensor([3,3], pt.format([dense, compressed]))\nB = pt.tensor([3,3], pt.format([dense, compressed]))\nC = pt.tensor([3,3], pt.format([dense, compressed], [1, 0]))\ni, j = pt.get_index_vars(2)\n\nA[i,j] = B[i,j] + C[i,j]  # ERROR!!  We can again perform the same computation by invoking  transpose , this time to\nrepack  C  into the same CSR format as  A  and  B  before computing the \naddition:  A = pt.tensor([3,3], pt.format([dense, compressed]))\nB = pt.tensor([3,3], pt.format([dense, compressed]))\nC = pt.tensor([3,3], pt.format([dense, compressed], [1, 0]))\ni, j = pt.get_index_vars(2)\n\nCp = C.transpose([0, 1], pt.format([dense, compressed]))  # Store a copy of C in the CSR format\nA[i,j] = B[i,j] + Cp[i,j]", 
            "title": "Expressing Transposes"
        }, 
        {
            "location": "/pycomputations/index.html#performing-the-computation", 
            "text": "Once a tensor algebra computation has been defined, you can simply invoke the\nresult tensor's  evaluate  method to perform the actual computation:  A.evaluate()  Under the hood, TACO will first invoke the result tensor's  compile \nmethod to generate code that performs the computation.  TACO will then perform \nthe actual computation by first invoking  assemble  to compute the sparsity \nstructure of the result and subsequently invoking  compute  to compute the \nvalues of the result's nonzero elements.  Of course, you can also manually \ninvoke these methods in order to more precisely control when each step happens:  A.compile()\nA.assemble()\nA.compute()  If you define a computation and then access the result without first manually\ninvoking  evaluate  or  compile / assemble / compute , TACO will automatically\ninvoke the computation immediately before the result is accessed.  In the\nfollowing example, for instance, TACO will automatically generate code to\ncompute the vector addition and then also actually perform the computation\nright before  a[0]  is printed:  a[i] = b[i] + c[i]\nprint(a[0])", 
            "title": "Performing the Computation"
        }, 
        {
            "location": "/pyreference/index.html", 
            "text": "Redirecting to reference manual...", 
            "title": "Reference Manual"
        }, 
        {
            "location": "/scientific_computing/index.html", 
            "text": "Sparse matrix-vector multiplication (SpMV) is a bottleneck computation in many\nscientific and engineering computations. Mathematically, SpMV can be expressed\nas \n\n\n\n\ny = Ax + z,\n\n\n\n\nwhere \nA\n is a sparse matrix and \nx\n, \ny\n, and \nz\n are dense vectors.\nThe computation can also be expressed in \nindex\nnotation\n as \n\n\n\n\ny_i = A_{ij} \\cdot x_j + z_i.\n\n\n\n\nYou can use the TACO C++ library to easily and efficiently compute SpMV, as\nshown here:\n\n\n// On Linux and MacOS, you can compile and run this program like so:\n//   g++ -std=c++11 -O3 -DNDEBUG -DTACO -I ../../include -L../../build/lib spmv.cpp -o spmv -ltaco\n//   LD_LIBRARY_PATH=../../build/lib ./spmv\n#include \nrandom\n\n#include \"taco.h\"\nusing namespace taco;\nint main(int argc, char* argv[]) {\n  std::default_random_engine gen(0);\n  std::uniform_real_distribution\ndouble\n unif(0.0, 1.0);\n  // Predeclare the storage formats that the inputs and output will be stored as.\n  // To define a format, you must specify whether each dimension is dense or sparse \n  // and (optionally) the order in which dimensions should be stored. The formats \n  // declared below correspond to compressed sparse row (csr) and dense vector (dv). \n  Format csr({Dense,Sparse});\n  Format  dv({Dense});\n\n  // Load a sparse matrix from file (stored in the Matrix Market format) and \n  // store it as a compressed sparse row matrix. Matrices correspond to order-2 \n  // tensors in taco. The matrix in this example can be downloaded from:\n  // https://www.cise.ufl.edu/research/sparse/MM/Boeing/pwtk.tar.gz\n  Tensor\ndouble\n A = read(\"pwtk.mtx\", csr);\n\n  // Generate a random dense vector and store it in the dense vector format. \n  // Vectors correspond to order-1 tensors in taco.\n  Tensor\ndouble\n x({A.getDimension(1)}, dv);\n  for (int i = 0; i \n x.getDimension(0); ++i) {\n    x.insert({i}, unif(gen));\n  }\n  x.pack();\n\n  // Generate another random dense vetor and store it in the dense vector format..\n  Tensor\ndouble\n z({A.getDimension(0)}, dv);\n  for (int i = 0; i \n z.getDimension(0); ++i) {\n    z.insert({i}, unif(gen));\n  }\n  z.pack();\n\n  // Declare and initializing the scaling factors in the SpMV computation. \n  // Scalars correspond to order-0 tensors in taco.\n  Tensor\ndouble\n alpha(42.0);\n  Tensor\ndouble\n beta(33.0);\n\n  // Declare the output matrix to be a sparse matrix with the same dimensions as \n  // input matrix B, to be also stored as a doubly compressed sparse row matrix.\n  Tensor\ndouble\n y({A.getDimension(0)}, dv);\n  // Define the SpMV computation using index notation.\n  IndexVar i, j;\n  y(i) = alpha() * (A(i,j) * x(j)) + beta() * z(i);\n  // At this point, we have defined how entries in the output vector should be \n  // computed from entries in the input matrice and vectorsbut have not actually \n  // performed the computation yet. To do so, we must first tell taco to generate \n  // code that can be executed to compute the SpMV operation.\n  y.compile();\n  // We can now call the functions taco generated to assemble the indices of the \n  // output vector and then actually compute the SpMV.\n  y.assemble();\n  y.compute();\n  // Write the output of the computation to file (stored in the FROSTT format).\n  write(\"y.tns\", y);\n}\n\n\n\nYou can also use the TACO Python library to perform the same computation, as\ndemonstrated here:\n\n\nimport pytaco as pt\nfrom pytaco import compressed, dense\nimport numpy as np\n\n# Define formats for storing the sparse matrix and dense vectors\ncsr = pt.format([dense, compressed])\ndv  = pt.format([dense])\n\n# Load a sparse matrix stored in the matrix market format) and store it \n# as a CSR matrix.  The matrix in this example can be downloaded from:\n# https://www.cise.ufl.edu/research/sparse/MM/Boeing/pwtk.tar.gz\nA = pt.read(\"pwtk.mtx\", csr)\n\n# Generate two random vectors using NumPy and pass them into TACO\nx = pt.from_array(np.random.uniform(size=A.shape[1]))\nz = pt.from_array(np.random.uniform(size=A.shape[0]))\n\n# Declare the result to be a dense vector\ny = pt.tensor([A.shape[0]], dv)\n\n# Declare index vars\ni, j = pt.get_index_vars(2)\n\n# Define the SpMV computation\ny[i] = A[i, j] * x[j] + z[i]\n\n# Perform the SpMV computation and write the result to file\npt.write(\"y.tns\", y)\n\n\n\nWhen you run the above Python program, TACO will generate code under the hood\nthat efficiently performs the computation in one shot.  This lets TACO avoid \nmaterializing the intermediate matrix-vector product, thus reducing the amount \nof memory accesses and speeding up the computation.", 
            "title": "Scientific Computing: SpMV"
        }, 
        {
            "location": "/data_analytics/index.html", 
            "text": "Matricized tensor times Khatri-Rao product (MTTKRP) is a bottleneck operation\nin various algorithms - such as Alternating Least Squares - for computing\nsparse tensor factorizations like the Canonical Polyadic Decomposition.\nMathematically, mode-1 MTTKRP (for three-dimensional tensors) can be expressed \nas \n\n\n\n\nA = B_{(1)} (D \\odot C),\n\n\n\n\nwhere \nA\n, \nC\n, and \nD\n are typically dense matrices, \nB\n is a\nthree-dimensional tensor (matricizied along the first mode), and \n\\odot\n\ndenotes the Khatri-Rao product. This operation can also be expressed in \nindex\nnotation\n as \n\n\n\n\nA_{ij} = B_{ikl} \\cdot D_{lj} \\cdot C_{kj}.\n\n\n\n\nYou can use the TACO C++ library to easily and efficiently compute the MTTKRP,\nas shown here:\n\n// On Linux and MacOS, you can compile and run this program like so:\n//   g++ -std=c++11 -O3 -DNDEBUG -DTACO -I ../../include -L../../build/lib mttkrp.cpp -o mttkrp -ltaco\n//   LD_LIBRARY_PATH=../../build/lib ./mttkrp\n#include \nrandom\n\n#include \"taco.h\"\nusing namespace taco;\nint main(int argc, char* argv[]) {\n  std::default_random_engine gen(0);\n  std::uniform_real_distribution\ndouble\n unif(0.0, 1.0);\n  // Predeclare the storage formats that the inputs and output will be stored as.\n  // To define a format, you must specify whether each dimension is dense or \n  // sparse and (optionally) the order in which dimensions should be stored. The \n  // formats declared below correspond to compressed sparse fiber (csf) and \n  // row-major dense (rm).\n  Format csf({Sparse,Sparse,Sparse});\n  Format  rm({Dense,Dense});\n\n  // Load a sparse order-3 tensor from file (stored in the FROSTT format) and \n  // store it as a compressed sparse fiber tensor. The tensor in this example \n  // can be download from: http://frostt.io/tensors/nell-2/\n  Tensor\ndouble\n B = read(\"nell-2.tns\", csf);\n  // Generate a random dense matrix and store it in row-major (dense) format. \n  // Matrices correspond to order-2 tensors in taco.\n  Tensor\ndouble\n C({B.getDimension(1), 25}, rm);\n  for (int i = 0; i \n C.getDimension(0); ++i) {\n    for (int j = 0; j \n C.getDimension(1); ++j) {\n      C.insert({i,j}, unif(gen));\n    }\n  }\n  C.pack();\n\n\n  // Generate another random dense matrix and store it in row-major format.\n  Tensor\ndouble\n D({B.getDimension(2), 25}, rm);\n  for (int i = 0; i \n D.getDimension(0); ++i) {\n    for (int j = 0; j \n D.getDimension(1); ++j) {\n      D.insert({i,j}, unif(gen));\n    }\n  }\n  D.pack();\n\n    // Declare the output matrix to be a dense matrix with 25 columns and the same\n  // number of rows as the number of slices along the first dimension of input\n  // tensor B, to be also stored as a row-major dense matrix.\n  Tensor\ndouble\n A({B.getDimension(0), 25}, rm);\n\n\n  // Define the MTTKRP computation using index notation.\n  IndexVar i, j, k, l;\n  A(i,j) = B(i,k,l) * D(l,j) * C(k,j);\n  // At this point, we have defined how entries in the output matrix should be\n  // computed from entries in the input tensor and matrices but have not actually\n  // performed the computation yet. To do so, we must first tell taco to generate\n  // code that can be executed to compute the MTTKRP operation.\n  A.compile();\n  // We can now call the functions taco generated to assemble the indices of the\n  // output matrix and then actually compute the MTTKRP.\n  A.assemble();\n  A.compute();\n  // Write the output of the computation to file (stored in the FROSTT format).\n  write(\"A.tns\", A);\n}\n\n\nYou can also use the TACO Python library to perform the same computation, as\ndemonstrated here:\n\n\nimport pytaco as pt\nimport numpy as np\nfrom pytaco import compressed, dense\n\n# Define formats for storing the sparse tensor and dense matrices\ncsf = pt.format([compressed, compressed, compressed])\nrm  = pt.format([dense, dense])\n\n# Load a sparse three-dimensional tensor from file (stored in the FROSTT\n# format) and store it as a compressed sparse fiber tensor. The tensor in this\n# example can be download from: http://frostt.io/tensors/nell-2/\nB = pt.read(\"nell-2.tns\", csf);\n\n# Generate two random matrices using NumPy and pass them into TACO\nC = pt.from_array(np.random.uniform(size=(B.shape[1], 25)))\nD = pt.from_array(np.random.uniform(size=(B.shape[2], 25)))\n\n# Declare the result to be a dense matrix\nA = pt.tensor([B.shape[0], 25], rm)\n\n# Declare index vars\ni, j, k, l = get_index_vars(4)\n\n# Define the MTTKRP computation\nA[i, j] = B[i, k, l] * D[l, j] * C[k, j]\n\n# Perform the MTTKRP computation and write the result to file\npt.write(\"A.tns\", A)\n\n\n\nWhen you run the above Python program, TACO will generate code under the hood\nthat efficiently performs the computation in one shot.  This lets TACO avoid\nmaterializing the intermediate Khatri-Rao product, thus reducing the amount of\nmemory accesses and speeding up the computation.", 
            "title": "Data Analytics: MTTKRP"
        }, 
        {
            "location": "/machine_learning/index.html", 
            "text": "Sampled dense-dense matrix product (SDDMM) is a bottleneck operation in many\nfactor analysis algorithms used in machine learning, including Alternating\nLeast Squares and Latent Dirichlet Allocation [1]. Mathematically, the\noperation can be expressed as \n\n\n\n\nA = B \\circ CD,\n\n\n\n\nwhere \nA\n and \nB\n are sparse matrices, \nC\n and \nD\n are dense matrices,\nand \n\\circ\n denotes component-wise multiplication. This operation can also be\nexpressed in \nindex\nnotation\n as \n\n\n\n\nA_{ij} = B_{ij} \\cdot C_{ik} \\cdot D_{kj}.\n\n\n\n\nYou can use the taco C++ library to easily and efficiently compute the SDDMM, as\nshown here:\n\n\n// On Linux and MacOS, you can compile and run this program like so:\n//   g++ -std=c++11 -O3 -DNDEBUG -DTACO -I ../../include -L../../build/lib sddmm.cpp -o sddmm -ltaco\n//   LD_LIBRARY_PATH=../../build/lib ./sddmm\n#include \nrandom\n\n#include \"taco.h\"\nusing namespace taco;\nint main(int argc, char* argv[]) {\n  std::default_random_engine gen(0);\n  std::uniform_real_distribution\ndouble\n unif(0.0, 1.0);\n  // Predeclare the storage formats that the inputs and output will be stored as.\n  // To define a format, you must specify whether each dimension is dense or sparse\n  // and (optionally) the order in which dimensions should be stored. The formats\n  // declared below correspond to doubly compressed sparse row (dcsr), row-major\n  // dense (rm), and column-major dense (dm).\n  Format dcsr({Sparse,Sparse});\n  Format   rm({Dense,Dense});\n  Format   cm({Dense,Dense}, {1,0});\n\n  // Load a sparse matrix from file (stored in the Matrix Market format) and\n  // store it as a doubly compressed sparse row matrix. Matrices correspond to\n  // order-2 tensors in taco. The matrix in this example can be download from:\n  // https://www.cise.ufl.edu/research/sparse/MM/Williams/webbase-1M.tar.gz\n  Tensor\ndouble\n B = read(\"webbase-1M.mtx\", dcsr);\n  // Generate a random dense matrix and store it in row-major (dense) format.\n  Tensor\ndouble\n C({B.getDimension(0), 1000}, rm);\n  for (int i = 0; i \n C.getDimension(0); ++i) {\n    for (int j = 0; j \n C.getDimension(1); ++j) {\n      C.insert({i,j}, unif(gen));\n    }\n  }\n  C.pack();\n\n  // Generate another random dense matrix and store it in column-major format.\n  Tensor\ndouble\n D({1000, B.getDimension(1)}, cm);\n  for (int i = 0; i \n D.getDimension(0); ++i) {\n    for (int j = 0; j \n D.getDimension(1); ++j) {\n      D.insert({i,j}, unif(gen));\n    }\n  }\n  D.pack();\n\n  // Declare the output matrix to be a sparse matrix with the same dimensions as\n  // input matrix B, to be also stored as a doubly compressed sparse row matrix.\n  Tensor\ndouble\n A(B.getDimensions(), dcsr);\n\n  // Define the SDDMM computation using index notation.\n  IndexVar i, j, k;\n  A(i,j) = B(i,j) * C(i,k) * D(k,j);\n\n  // At this point, we have defined how entries in the output matrix should be\n  // computed from entries in the input matrices but have not actually performed\n  // the computation yet. To do so, we must first tell taco to generate code that\n  // can be executed to compute the SDDMM operation.\n  A.compile();\n  // We can now call the functions taco generated to assemble the indices of the\n  // output matrix and then actually compute the SDDMM.\n  A.assemble();\n  A.compute();\n  // Write the output of the computation to file (stored in the Matrix Market format).\n  write(\"A.mtx\", A);\n}\n\n\n\nYou can also use the TACO Python library to perform the same computation, as\ndemonstrated here:\n\n\nimport pytaco as pt\nfrom pytaco import dense, compressed\nimport numpy as np\n\n# Define formats that the inputs and output will be stored as.  To define a\n# format, you must specify whether each dimension is dense or sparse and\n# (optionally) the order in which dimensions should be stored. The formats\n# declared below correspond to doubly compressed sparse row (dcsr), row-major\n# dense (rm), and column-major dense (dm).\ndcsr = pt.format([compressed, compressed])\nrm   = pt.format([dense, dense])\ncm   = pt.format([dense, dense], [1, 0])\n\n# The matrix in this example can be download from:\n# https://www.cise.ufl.edu/research/sparse/MM/Williams/webbase-1M.tar.gz\nB = pt.read(\"webbase-1M.mtx\", dcsr)\n\n# Generate two random matrices using NumPy and pass them into TACO\nx = pt.from_array(np.random.uniform(size=(B.shape[0], 1000)))\nz = pt.from_array(np.random.uniform(size=(1000, B.shape[1])), out_format=cm)\n\n# Declare the result to be a doubly compressed sparse row matrix\nA = pt.tensor(B.shape, dcsr)\n\n# Declare index vars\ni, j, k = pt.get_index_vars(3)\n\n# Define the SDDMM computation\nA[i, j] = B[i, j] * C[i, k] * D[k, j]\n\n# Perform the SDDMM computation and write the result to file\npt.write(\"A.mtx\", A)\n\n\n\nWhen you run the above Python program, TACO will generate code under the hood\nthat efficiently performs the computation in one shot.  This lets TACO only \ncompute elements of the intermediate dense matrix product that are actually \nneeded to compute the result, thus reducing the asymptotic complexity of the \ncomputation.\n\n\n[1] Huasha Zhao. 2014. High Performance Machine Learning through Codesign and\nRooflining. Ph.D. Dissertation. EECS Department, University of California,\nBerkeley.", 
            "title": "Machine Learning: SDDMM"
        }, 
        {
            "location": "/optimization/index.html", 
            "text": "This section describes various strategies for improving the performace of\napplications that use TACO to perform linear and tensor algebra computations.\n\n\nSelecting the Right Tensor Format\n\n\nTACO supports storing tensors in a wide range of formats, including many\ncommonly used ones like dense arrays, compressed sparse row (CSR), and\ncompressed sparse fiber (CSF).  Using the right formats to store a sparse\ncomputation's operands and result can not only reduce the amount of memory\nneeded to perform the computation but also improve its performance.  In\nparticular, by selecting formats that accurately describe the sparsity and\nstructure of the operands, TACO can generate code under the hood that exploits\nthese properties of the data to avoid redundantly computing with zero elements\nand thus speed up a computation.\n\n\nAs previously \nexplained\n, TACO uses a\nnovel scheme that describes different tensor storage formats by specifying\nwhether each dimension is sparse or dense.  A dense dimension indicates to TACO\nthat most if not all slices of the tensor along that dimension contain at least\none nonzero element.  So if every element in a matrix is nonzero, we can make\nthat explicit by storing the matrix in a format where both dimensions are\ndense, which indicates that every row is nonempty and that every column in each\nrow stores a nonzero element:\n\n\npytaco.format([pytaco.dense, pytaco.dense])  # a.k.a. a dense array\n\n\n\nA sparse dimension, on the other hand, indicates to TACO that most slices of\nthe tensor along that dimension contain only zeros.  So if relatively few rows\nof a matrix is nonempty and if relatively few columns in each nonempty row\nstore nonzero elements, we can also make that explicit by storing the matrix in\na format where both dimensions are sparse:\n\n\npytaco.format([pytaco.compressed, pytaco.compressed])  # a.k.a. a DCSR matrix\n\n\n\n\n\nTip\n\n\nStoring a tensor dimension as a sparse dimension incurs overhead that is \nproportional to the number of nonempty slices along that dimension, so only \ndo so if most slices are actually empty.  Otherwise, it is more appropriate \nto store the dimension as a dense dimension.\n\n\n\n\nIt is easy to define custom formats for storing tensors with complex\nsparsity structures.  For example, let's say we have a three-dimensional\ntensor \nA_{ijk}\n that has no empty slice along the \nK\n dimension, and let's\nsay that each row in a slice is either entirely empty (i.e., \nA_{ijk} = 0\n\nfor all \nj\n and some fixed \nk\n, \ni\n) or entirely full (i.e., \nA_{ijk}\n\\neq 0\n for all \nj\n and some fixed \nk\n, \ni\n).  Following the same scheme\nas before, we can define a tensor format that stores dimension 2 (i.e., the\n\nK\n dimension) as a dense dimension, stores dimension 0 (i.e., the \nI\n\ndimension) of each slice along dimension 2 as a sparse dimension, and stores\ndimension 1 (i.e., the \nJ\n dimension) of each nonempty row as a dense\ndimension also:\n\n\npytaco.format([pytaco.dense, pytaco.compressed, pytaco.dense], [2, 0, 1])\n\n\n\nUsing the format above, we can then efficiently store \nA\n without explicitly \nstoring any zero element.\n\n\nAs a rough rule of thumb, using formats that accurately describe the sparsity\nand structure of the operands lets TACO minimize memory traffic incurred to\nload tensors from memory as well as minimize redundant work done to perform a\ncomputation, which boosts performance.  This is particularly the case when only\none operand is sparse and the computation does not involve adding elements of\nmultiple operands.  \nThis is not a hard and fast rule though.\n  In\nparticular, computing with multiple sparse operands might prevent TACO from \napplying some optimizations like \nparallelization\n \nthat might otherwise be possible if some of those operands were stored in dense \nformats.  Depending on how sparse your data actually is, this may or may not \nnegatively impact performance.\n\n\nThe most reliable way to determine what are the best formats for storing\ntensors in your application is to just try out many different formats and see\nwhat works. Fortunately, as the examples above demonstrate, this is simple to\ndo with TACO.\n\n\nParallelizing Computations\n\n\nBy default, TACO performs all computations using a single thread.  The maximum\nnumber of threads that TACO may use to perform computations can be adjusted by\ncalling the \npytaco.set_num_threads\n function.  The example below, for\ninstance, tells TACO that up to four threads may be used to execute any\nsubsequent computation in parallel if possible:\n\n\npytaco.set_num_threads(4)\n\n\n\nIn general, the maximum number of threads for performing computations should\nnot be set greater than the number of available processor cores.  And depending\non the specific computation and characteristics of the data, setting the\nmaximum number of threads to be less than the number of processor cores may\nactually yield better performance.  As the example above demonstrates, TACO \nmakes it easy to try out different numbers of threads and see what works best \nfor your application.\n\n\n\n\nNote\n\n\nSetting the maximum number of available threads to be greater than one does\nnot guarantee that all computations will be executed in parallel.  In\nparticular, TACO will not execute a computation in parallel if \n\n\n\n\nit multiplies two or more sparse operands (e.g. sparse vector\n  multiplication) or adds a sparse operand to any other operand (e.g.,\n  adding two DCSR matrices), unless the outermost dimensions of the sparse\n  operands are stored as dense dimensions (e.g., adding two CSR matrices,\n  which can be parallelized if the result is stored in a dense matrix); \n\n\nthe first dimension it has to iterate over is one that is supposed to\n  reduce over (e.g., multiplying a CSC matrix by a vector, which requires\n  iterating over the column dimension of the matrix before the row\n  dimension even though the column dimension is reduced over); or\n\n\nit stores the result in a sparse tensor format.\n\n\n\n\nIf TACO does not seem to be executing a computation in parallel, using \ndifferent formats to store the operands and result may help.\n\n\n\n\nBy default, when performing computations in parallel, TACO will assign the same\nnumber of coordinates along a particular dimension to be processed by each\nthread.  For instance, when adding two 1000-by-1000 matrices using two threads,\nTACO will have each thread compute exactly 500 rows of the result.  This would \nbe inefficient though if, for instance, all the nonzeros are stored in the\nfirst 500 rows of the operands, since one thread would end up doing all the\nwork while the other thread does nothing.  In cases like this, an alternative\nparallelization strategy can be specified by calling the\n\npytaco.set_parallel_schedule\n function:\n\n\npt.set_parallel_schedule(\"dynamic\") \n\n\n\nIn contrast to the default parallelization strategy, the dynamic strategy will\nhave each thread first compute just one row of the result.  Whenever a thread\nfinishes computing a row, TACO will assign another row for that thread to\ncompute, and this process is repeated until all 1000 rows have been computed.\nIn this way, work is guaranteed to be evenly distributed between the two\nthreads regardless of the sparsity structures of the operands.\n\n\nUsing a dynamic strategy for parallel execution will incur some overhead\nthough, since work is assigned to threads at runtime.  This overhead can be\nreduced by increasing the chunk size, which is the amount of additional work\nthat is assigned to a thread whenever it completes its previously assigned\nwork.  The example below, for instance, tells TACO to assign ten additional\nrows of the result (instead of just one) for a thread to compute whenever it\nhas completed the previous ten rows:\n\n\npt.set_parallel_schedule(\"dynamic\", 10) \n\n\n\nSince dynamic parallelization strategies incur additional overhead, whether or\nnot using them improves the performance of a computation will depend on how\nevenly spread out the nonzero elements in the tensor operands are.  If each\nmatrix contains roughly the same number of nonzeros in every row, for instance,\nthen using a dynamic strategy will likely not more evenly distribute work\nbetween threads.  In that case, the default static schedule would likely yield\nbetter performance.\n\n\nFusing Computations\n\n\nTACO supports efficiently computing complicated tensor algebra expressions\ninvolving many discrete operations in a single shot.  Let's say, for instance,\nthat we would like to (element-wise) add two vectors \nb\n and \nc\n and compute\nthe cosine of each element in the sum.  We can, of course, simply compute the\naddition and the cosine of the sum in separate statements:\n\n\nt[i] = b[i] + c[i]\na[i] = pt.cos(t[i])\n\n\n\nThe program above will first invoke TACO to add \nb\n and \nc\n, store the result\ninto a temporary vector \nt\n, and then invoke TACO again to compute the cosine\nof every element in \nt\n.  Performing the computation this way though not only\nrequires additional memory for storing \nt\n but also requires accessing the\nmemory subsystem to first write \nt\n to memory and then load \nt\n back from\nmemory, which is inefficient if the vectors are large and cannot be stored in\ncache.  Instead, we can compute the addition and the cosine of the sum in a \nsingle statement:\n\n\na[i] = pt.cos(b[i] + c[i])\n\n\n\nFor the program above, TACO will automatically generate code that, for every\n\ni\n, immediately computes the cosine of \nb[i] + c[i]\n as soon as the sum is\ncomputed.  TACO thus avoids storing the sum of \nb\n and \nc\n in a temporary\nvector, thereby increasing the performance of the computation.\n\n\nFusing computations can improve performance if it does not require intermediate\nresults to be recomputed multiple times, as is the case with the previous\nexample.  Let's say, however, that we would like to multiply a matrix \nB\n by a\nvector \nc\n and then multiply another matrix \nA\n by the result of the first\nmultiplication.  As before, we can express both operations in a single\nstatement:\n\n\ny[i] = A[i,j] * B[j,k] * x[k]\n\n\n\nIn this case though, computing both operations in one shot would require that\nthe multiplication of \nB\n and \nx\n be redundantly recomputed for every\n(non-empty) row of \nA\n, thus reducing performance.  By contrast, computing the\ntwo matrix-vector multiplications in separate statement ensures that the result\nof the first matrix-vector multiplication does not have to be redundantly\ncomputed, thereby minimizing the amount of work needed to perform the\ncomputation:\n\n\nt[j] = B[j,k] * c[k]\ny[i] = A[i,j] * t[j]", 
            "title": "Strategies for Optimization"
        }, 
        {
            "location": "/optimization/index.html#selecting-the-right-tensor-format", 
            "text": "TACO supports storing tensors in a wide range of formats, including many\ncommonly used ones like dense arrays, compressed sparse row (CSR), and\ncompressed sparse fiber (CSF).  Using the right formats to store a sparse\ncomputation's operands and result can not only reduce the amount of memory\nneeded to perform the computation but also improve its performance.  In\nparticular, by selecting formats that accurately describe the sparsity and\nstructure of the operands, TACO can generate code under the hood that exploits\nthese properties of the data to avoid redundantly computing with zero elements\nand thus speed up a computation.  As previously  explained , TACO uses a\nnovel scheme that describes different tensor storage formats by specifying\nwhether each dimension is sparse or dense.  A dense dimension indicates to TACO\nthat most if not all slices of the tensor along that dimension contain at least\none nonzero element.  So if every element in a matrix is nonzero, we can make\nthat explicit by storing the matrix in a format where both dimensions are\ndense, which indicates that every row is nonempty and that every column in each\nrow stores a nonzero element:  pytaco.format([pytaco.dense, pytaco.dense])  # a.k.a. a dense array  A sparse dimension, on the other hand, indicates to TACO that most slices of\nthe tensor along that dimension contain only zeros.  So if relatively few rows\nof a matrix is nonempty and if relatively few columns in each nonempty row\nstore nonzero elements, we can also make that explicit by storing the matrix in\na format where both dimensions are sparse:  pytaco.format([pytaco.compressed, pytaco.compressed])  # a.k.a. a DCSR matrix   Tip  Storing a tensor dimension as a sparse dimension incurs overhead that is \nproportional to the number of nonempty slices along that dimension, so only \ndo so if most slices are actually empty.  Otherwise, it is more appropriate \nto store the dimension as a dense dimension.   It is easy to define custom formats for storing tensors with complex\nsparsity structures.  For example, let's say we have a three-dimensional\ntensor  A_{ijk}  that has no empty slice along the  K  dimension, and let's\nsay that each row in a slice is either entirely empty (i.e.,  A_{ijk} = 0 \nfor all  j  and some fixed  k ,  i ) or entirely full (i.e.,  A_{ijk}\n\\neq 0  for all  j  and some fixed  k ,  i ).  Following the same scheme\nas before, we can define a tensor format that stores dimension 2 (i.e., the K  dimension) as a dense dimension, stores dimension 0 (i.e., the  I \ndimension) of each slice along dimension 2 as a sparse dimension, and stores\ndimension 1 (i.e., the  J  dimension) of each nonempty row as a dense\ndimension also:  pytaco.format([pytaco.dense, pytaco.compressed, pytaco.dense], [2, 0, 1])  Using the format above, we can then efficiently store  A  without explicitly \nstoring any zero element.  As a rough rule of thumb, using formats that accurately describe the sparsity\nand structure of the operands lets TACO minimize memory traffic incurred to\nload tensors from memory as well as minimize redundant work done to perform a\ncomputation, which boosts performance.  This is particularly the case when only\none operand is sparse and the computation does not involve adding elements of\nmultiple operands.   This is not a hard and fast rule though.   In\nparticular, computing with multiple sparse operands might prevent TACO from \napplying some optimizations like  parallelization  \nthat might otherwise be possible if some of those operands were stored in dense \nformats.  Depending on how sparse your data actually is, this may or may not \nnegatively impact performance.  The most reliable way to determine what are the best formats for storing\ntensors in your application is to just try out many different formats and see\nwhat works. Fortunately, as the examples above demonstrate, this is simple to\ndo with TACO.", 
            "title": "Selecting the Right Tensor Format"
        }, 
        {
            "location": "/optimization/index.html#parallelizing-computations", 
            "text": "By default, TACO performs all computations using a single thread.  The maximum\nnumber of threads that TACO may use to perform computations can be adjusted by\ncalling the  pytaco.set_num_threads  function.  The example below, for\ninstance, tells TACO that up to four threads may be used to execute any\nsubsequent computation in parallel if possible:  pytaco.set_num_threads(4)  In general, the maximum number of threads for performing computations should\nnot be set greater than the number of available processor cores.  And depending\non the specific computation and characteristics of the data, setting the\nmaximum number of threads to be less than the number of processor cores may\nactually yield better performance.  As the example above demonstrates, TACO \nmakes it easy to try out different numbers of threads and see what works best \nfor your application.   Note  Setting the maximum number of available threads to be greater than one does\nnot guarantee that all computations will be executed in parallel.  In\nparticular, TACO will not execute a computation in parallel if    it multiplies two or more sparse operands (e.g. sparse vector\n  multiplication) or adds a sparse operand to any other operand (e.g.,\n  adding two DCSR matrices), unless the outermost dimensions of the sparse\n  operands are stored as dense dimensions (e.g., adding two CSR matrices,\n  which can be parallelized if the result is stored in a dense matrix);   the first dimension it has to iterate over is one that is supposed to\n  reduce over (e.g., multiplying a CSC matrix by a vector, which requires\n  iterating over the column dimension of the matrix before the row\n  dimension even though the column dimension is reduced over); or  it stores the result in a sparse tensor format.   If TACO does not seem to be executing a computation in parallel, using \ndifferent formats to store the operands and result may help.   By default, when performing computations in parallel, TACO will assign the same\nnumber of coordinates along a particular dimension to be processed by each\nthread.  For instance, when adding two 1000-by-1000 matrices using two threads,\nTACO will have each thread compute exactly 500 rows of the result.  This would \nbe inefficient though if, for instance, all the nonzeros are stored in the\nfirst 500 rows of the operands, since one thread would end up doing all the\nwork while the other thread does nothing.  In cases like this, an alternative\nparallelization strategy can be specified by calling the pytaco.set_parallel_schedule  function:  pt.set_parallel_schedule(\"dynamic\")   In contrast to the default parallelization strategy, the dynamic strategy will\nhave each thread first compute just one row of the result.  Whenever a thread\nfinishes computing a row, TACO will assign another row for that thread to\ncompute, and this process is repeated until all 1000 rows have been computed.\nIn this way, work is guaranteed to be evenly distributed between the two\nthreads regardless of the sparsity structures of the operands.  Using a dynamic strategy for parallel execution will incur some overhead\nthough, since work is assigned to threads at runtime.  This overhead can be\nreduced by increasing the chunk size, which is the amount of additional work\nthat is assigned to a thread whenever it completes its previously assigned\nwork.  The example below, for instance, tells TACO to assign ten additional\nrows of the result (instead of just one) for a thread to compute whenever it\nhas completed the previous ten rows:  pt.set_parallel_schedule(\"dynamic\", 10)   Since dynamic parallelization strategies incur additional overhead, whether or\nnot using them improves the performance of a computation will depend on how\nevenly spread out the nonzero elements in the tensor operands are.  If each\nmatrix contains roughly the same number of nonzeros in every row, for instance,\nthen using a dynamic strategy will likely not more evenly distribute work\nbetween threads.  In that case, the default static schedule would likely yield\nbetter performance.", 
            "title": "Parallelizing Computations"
        }, 
        {
            "location": "/optimization/index.html#fusing-computations", 
            "text": "TACO supports efficiently computing complicated tensor algebra expressions\ninvolving many discrete operations in a single shot.  Let's say, for instance,\nthat we would like to (element-wise) add two vectors  b  and  c  and compute\nthe cosine of each element in the sum.  We can, of course, simply compute the\naddition and the cosine of the sum in separate statements:  t[i] = b[i] + c[i]\na[i] = pt.cos(t[i])  The program above will first invoke TACO to add  b  and  c , store the result\ninto a temporary vector  t , and then invoke TACO again to compute the cosine\nof every element in  t .  Performing the computation this way though not only\nrequires additional memory for storing  t  but also requires accessing the\nmemory subsystem to first write  t  to memory and then load  t  back from\nmemory, which is inefficient if the vectors are large and cannot be stored in\ncache.  Instead, we can compute the addition and the cosine of the sum in a \nsingle statement:  a[i] = pt.cos(b[i] + c[i])  For the program above, TACO will automatically generate code that, for every i , immediately computes the cosine of  b[i] + c[i]  as soon as the sum is\ncomputed.  TACO thus avoids storing the sum of  b  and  c  in a temporary\nvector, thereby increasing the performance of the computation.  Fusing computations can improve performance if it does not require intermediate\nresults to be recomputed multiple times, as is the case with the previous\nexample.  Let's say, however, that we would like to multiply a matrix  B  by a\nvector  c  and then multiply another matrix  A  by the result of the first\nmultiplication.  As before, we can express both operations in a single\nstatement:  y[i] = A[i,j] * B[j,k] * x[k]  In this case though, computing both operations in one shot would require that\nthe multiplication of  B  and  x  be redundantly recomputed for every\n(non-empty) row of  A , thus reducing performance.  By contrast, computing the\ntwo matrix-vector multiplications in separate statement ensures that the result\nof the first matrix-vector multiplication does not have to be redundantly\ncomputed, thereby minimizing the amount of work needed to perform the\ncomputation:  t[j] = B[j,k] * c[k]\ny[i] = A[i,j] * t[j]", 
            "title": "Fusing Computations"
        }
    ]
}